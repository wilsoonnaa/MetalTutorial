{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Metal","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to the Metal Tutorial. This tutorial will teach you the basics of Apple's Metal Graphics and Compute API, and help you understand how to program with it in C++ via the <code>metal-cpp</code> library that Apple has now officially released. There isn't much documentation for it yet, and it is missing some features, so I'll show you how to work around those in the following chapters. For those who are completely new, I'll try to go over everything in as much detail as I possibly can, and link to other resources for more information as necessary. I hope that this guide can be of use to you. You can find the source code for each chapter in the series on the github repository. Additionally, if you would like to contribute your own content to this tutorial series, or correct any mistakes that I've made, please feel free to comment, create an issue, or submit a pull request.</p>"},{"location":"#metal-documenation-and-other-useful-resources","title":"Metal Documenation and Other Useful Resources","text":"<p>Here are some resources that you may find helpful both while completing these tutorials, as well as when using Metal more generally:</p>"},{"location":"#official-apple-metal-documentation","title":"Official Apple Metal Documentation","text":"<ul> <li>Metal Documentation</li> <li>Metal Specification</li> <li>Creating and Sampling Textures</li> <li>Creating Threads and Threadgroups</li> </ul>"},{"location":"#useful-metal-resources","title":"Useful Metal Resources","text":"<ul> <li>Intro to Metal Compute</li> <li>Constant vs Device Address Space</li> <li>\"Pass by reference\" in Metal</li> <li>Metal Best Practices Guide (Drawables)</li> <li>Metal-CPP discussion reddit</li> </ul>"},{"location":"#computer-graphics-fundamentals","title":"Computer Graphics Fundamentals","text":"<ul> <li>Linear Algebra</li> </ul>"},{"location":"#ray-tracing","title":"Ray Tracing","text":"<ul> <li>GPU Ray-Tracing in One an Afternoon</li> <li>GPU Ray-Tracing in One Weekend</li> <li>CUDA Compute Ray-Tracing</li> <li>GPU Accelerated Path-Tracer (hemispheres/rand)</li> <li>Random Number Generation and Sampling (like on hemisphere)</li> </ul>"},{"location":"Setup/","title":"Metal-cpp Setup","text":""},{"location":"Setup/#setting-up-xcode-for-use-with-metal-cpp","title":"Setting up Xcode for use with <code>metal-cpp</code>","text":"<p>In order to write Metal code, either for an iOS based device or MacOS, we're pretty much going to need Xcode. Development for Apple devices can be done in other editors, but it is not recommended, as Xcode is created by Apple specifically for development within its ecosystem. If you don't have it already, you can download it for free from the Mac App Store. </p> <p>If you don't want to go through the <code>metal-cpp</code> or GLFW setup process, head on over to the GitHub and clone Lesson 0 where I've configured the project already. Otherwise, read on!</p> <p>Metal is designed to be written in a programming language called Objective-C. Objective-C, along with Apple's Swift programming language, are the languages typically used to write macOS and iOS applications. In this tutorial series, however, we're going to be writing our application primarily in C++, as this is the industry standard for writing Computer Graphics related applications, whether it be for real-time rendering in video games, or for CAD software. I say \"primarily\" because we'll actually be using a bit of \"Objective-C++\", which allows us to mix Objective-C code with C++ to make things a little easier for us. Even though Metal is typically written in pure Objective-C, for convenience Apple has released a set of C++ bindings that act as an interface to the the Metal Graphics API, allowing us to write high-performance graphics and compute applications for Apple devices in (almost) pure C++. This wrapper is very light-weight, and acts as a one-to-one translation between the native Objective-C functions calls and C++. You're going to need to download the library here.</p> <p>Once you have downloaded the <code>metal-cpp</code> library, open Xcode and <code>Create a new Xcode project</code>. For this tutorial we're going to be targetting Mac devices, so under the <code>macOS</code> templates, select <code>Command Line Tool</code>. This will set the default language to C++ and give us an empty <code>Hello-World</code> project.</p> <p>The first thing we want to do is drag and drop our freshly downloaded and unzipped 'metal-cpp' folder into our Xcode project.</p> <p></p> <p>Now that we've got it copied over to our project, we need to make sure Xcode can find it. If we head over to the <code>Build Settings</code> section of our project target, under <code>Search Paths</code>, go ahead and add the metal-cpp folder to your header search paths: <pre><code>$(PROJECT_DIR)/metal-cpp\n</code></pre></p> <p></p> <p>The next thing we should do is link with the necessary Apple frameworks to be able to use Metal. Head over to the <code>Build Phases</code> section, and under <code>Link Binary With Libraries</code>, add these three frameworks: <pre><code>Foundation.framework\nMetal.framework\nQuartzCore.framework\n</code></pre></p> <p></p> <p>Now, Metal should be ready to go. Apple's <code>metal-cpp</code> guide tells us that we need to define the <code>metal-cpp</code> implementation in only one of our <code>.cpp</code> files. We're going to create a new file called <code>mtl_implementation.cpp</code> to do this for us, fill it with the necessary <code>define</code> and <code>include</code> statements:</p> <pre><code>//  mtl_implementation.cpp\n#define NS_PRIVATE_IMPLEMENTATION\n#define CA_PRIVATE_IMPLEMENTATION\n#define MTL_PRIVATE_IMPLEMENTATION\n#include &lt;Foundation/Foundation.hpp&gt;\n#include &lt;Metal/Metal.hpp&gt;\n#include &lt;QuartzCore/QuartzCore.hpp&gt;\n</code></pre> <p>Back in our <code>main.cpp</code> file, you add the Metal include, and create a default device:</p> <p><pre><code>//  main.cpp\n#include &lt;Metal/Metal.hpp&gt;\n...\nint main() {\n...\nMTL::Device* device = MTL::CreateSystemDefaultDevice();\n...\n}\n</code></pre> Now, you can build and run: </p> <p>If everything went smoothly, you should see some output in the Xcode console mentioning Metal API Validation and what not. If you don't, make sure you followed all of the steps correctly.</p>"},{"location":"Setup/#setting-up-glfw-for-window-creation","title":"Setting up GLFW for Window Creation","text":"<p>To use the graphics rendering aspect of Metal, we're going to need a window to render our application to. This is where a cross platform windowing library called <code>GLFW</code> comes in. Typically, it's used in conjunction with the OpenGL graphics API, but we can disable this functionality to be able to use it with Metal. In short, it's going to make window creation, as well as keyboard/mouse/controller input handling really easy for us. MacOS has it's own windowing library that's part of the Cocoa framework, but I'm not familiar with how to use it, and we'll able to setup GLFW and get it to expose the part of Cocoa windowing that we will need in about 5 lines of code.</p> <p>There's a couple of different ways to get GLFW. One way is to download the macOS pre-compiled binaries on the glfw website. This is the way I've set it up on the GitHub repository, starting with Lesson 0. You can head over there and clone that project if you'd like to use GLFW's pre-compiled universal binaries, supported on Intel as well as M1 and later Macs.</p> <p>Alternatively, you can get it via the <code>brew</code> package manager, which is what I'll show here. If you don't have <code>brew</code>, you can get it here: brew.sh. It's a very useful tool to have for software development on Mac. It's essentially like <code>apt-get</code> on Debian Linux distros, if you're familiar with that.</p> <p>Once you have brew, you can install glfw with this terminal command: <pre><code>brew install glfw\n</code></pre> Now, we need to add brew's header <code>include</code> directory to our Xcode header search path, so Xcode can find GLFW's header files. </p> <p>If you're not sure what kind of Mac you have, click the \uf8ff logo in the top left of your screen, hit <code>About This Mac</code> and check the Chip type.</p> <p>If you're on an M1 Mac, like I am, you can add this directory under the <code>Build Settings</code> section for your Xcode target: <pre><code>/opt/homebrew/Cellar/glfw/...version_goes_here.../include\n</code></pre> </p> <p>Alternatively, if you have an Intel Mac, brew will install packages to the <code>/usr/local</code> directory. Correspondingly, you can use this directory instead: <pre><code>/usr/local/Cellar/glfw/...version_goes_here.../include\n</code></pre></p> <p>Next, we're going to link with the glfw dynamic library. Under Build Phases, and under <code>Link Binary With Libraries</code>, click the <code>+</code> icon, <code>Add Other</code>, then <code>Add Files...</code>. It will open up a <code>Finder Window</code>: </p> <p>When you have the window open, hold these 3 keys at the same time: Cmd+Shift+G. If on an M1 Mac: <pre><code>/opt/homebrew/Cellar/glfw/...version_goes_here.../lib\n</code></pre> And if on an Intel Mac: <pre><code>/usr/local/Cellar/glfw/...version_goes_here.../lib\n</code></pre> Select <code>libglfw.3.3.dylib</code>: </p> <p>And you should be all set to use GLFW!</p>"},{"location":"Extra%20Chapters/Compute%20Shaders/","title":"Ray-Tracing with Compute Shaders","text":"<p>Once again, the full write-up is to come. For now, here's the final product:</p> <p></p>"},{"location":"Lesson%201%3A%20Hello%20Metal/1.%20Hello%20Window/","title":"Setting up our Window","text":"<p>Now that we've configured the <code>metal-cpp</code> library and GLFW, it's time to make a window, as we'll need one before we'll be able to render anything to our screen. We're going to stash all of our rendering engine logic in a new file we'll call <code>mtl_engine.mm</code>, with the corresponding header <code>mtl_engine.hpp</code>. You might be wondering what the <code>.mm</code> extension is for. This is where the Objective-C I discussed in the <code>Setup</code> section comes in. Essentially, it tells Xcode to allow us to combine C++ and Objective-C code in the same file and compile it. We have to do this, because some of the windowing functions we'll need aren't mapped to the <code>metal-cpp</code> library for some reason, at least at the time of writing this. Critically, we're going to want to change our <code>main.cpp</code> file to <code>main.mm</code> as well.</p> <p>In <code>mtl_engine.hpp</code>, we're first going to include the necessary headers for <code>GLFW</code> and <code>metal-cpp</code>: mtl_engine.hpp<pre><code>#pragma once\n#define GLFW_INCLUDE_NONE\n#import &lt;GLFW/glfw3.h&gt;\n#define GLFW_EXPOSE_NATIVE_COCOA\n#import &lt;GLFW/glfw3native.h&gt;\n#include &lt;Metal/Metal.hpp&gt;\n#include &lt;Metal/Metal.h&gt;\n#include &lt;QuartzCore/CAMetalLayer.hpp&gt;\n#include &lt;QuartzCore/CAMetalLayer.h&gt;\n#include &lt;QuartzCore/QuartzCore.hpp&gt;\n</code></pre> Notice the <code>#define GLFW_EXPOSE_NATIVE_COCOA</code> bit, which is a macro that exposes the native APIs of GLFW for the Cocoa framework on MacOS. It's critical to include that, as it's responsible for our windowing technique!</p> <p>Next, we'll define our Metal Engine class, <code>MTLEngine</code>: mtl_engine.hpp<pre><code>class MTLEngine {\npublic:\nvoid init();\nvoid run();\nvoid cleanup();\nprivate:\nvoid initDevice();\nvoid initWindow();\nMTL::Device* metalDevice;\nGLFWwindow* glfwWindow;\nNSWindow* metalWindow;\nCAMetalLayer* metalLayer;\n};\n</code></pre> We have three public functions that will be driven in our applications <code>main</code> function, <code>init()</code>, <code>run()</code>, and <code>cleanup()</code>. The <code>init()</code> function will do all of our Window and Metal setup. We're keeping it simple for now, just enough functionality to get a window opened. You'll notice 4 member variables defined at the bottom. The <code>MTL::Device* metalDevice</code> gives us access to our devices GPU, and the plethora of commands that it provides us for rendering and GPU compute work. <code>GLFWwindow* glfwWindow</code> gives us a handle to our GLFWwindow, as well as GLFW's various callback functions, like handling keyboard input and whatnot. For those who are unfamiliar, you can read more on GLFW's functionality at glfw.com. We also have our <code>NSWindow* metalWindow</code> and <code>CAMetalLayer* metalLayer</code>, which are actually our first two Objective-C variables. I'll explain their purpose when we create the window.</p> <p>First, let's take a look at the Engine implementation: mtl_engine.mm<pre><code>#include \"mtl_engine.hpp\"\nvoid MTLEngine::init() {\ninitDevice();\ninitWindow();\n}\nvoid MTLEngine::run() {\nwhile (!glfwWindowShouldClose(glfwWindow)) {\nglfwPollEvents();\n}\n}\nvoid MTLEngine::cleanup() {\nglfwTerminate();\nmetalDevice-&gt;release();\n}\nvoid MTLEngine::initDevice() {\nmetalDevice = MTL::CreateSystemDefaultDevice();\n}\nvoid MTLEngine::initWindow() {\nglfwInit();\nglfwWindowHint(GLFW_CLIENT_API, GLFW_NO_API);\nglfwWindow = glfwCreateWindow(800, 600, \"Metal Engine\", NULL, NULL);\nif (!glfwWindow) {\nglfwTerminate();\nexit(EXIT_FAILURE);\n}\nmetalWindow = glfwGetCocoaWindow(glfwWindow);\nmetalLayer = [CAMetalLayer layer];\nmetalLayer.device = (__bridge id&lt;MTLDevice&gt;)metalDevice;\nmetalLayer.pixelFormat = MTLPixelFormatBGRA8Unorm;\nmetalWindow.contentView.layer = metalLayer;\nmetalWindow.contentView.wantsLayer = YES;\n}\n</code></pre> It's only 38 lines, so it shouldn't be terribly confusing.</p> <p>mtl_engine.mm<pre><code>void MTLEngine::init() {\ninitDevice();\ninitWindow();\n}\n</code></pre> In the <code>init()</code> function, all we're going to be doing is calling our Metal Device and GLFW Window setup methods: <code>initDevice()</code> and <code>initWindow()</code>.</p> <p>mtl_engine.mm<pre><code>void MTLEngine::initDevice() {\nmetalDevice = MTL::CreateSystemDefaultDevice();\n}\n</code></pre> The contents of <code>initDevice()</code> should look familiar, we're simply using the <code>metal-cpp</code> library to create our Metal Device handle which gives us access to our GPU. We'll use our Metal Device for many things, but not limited to: </p> <ul> <li>Shader Library creation.</li> <li>Buffer and Texture Resource creation, and passing data betwen the CPU and GPU.</li> <li>Render and Compute Pipeline creation.</li> </ul> <p>We'll get into the details of what all these things mean in the following chapters as they come up.</p> <p>mtl_engine.mm<pre><code>void MTLEngine::initWindow() {\nglfwInit();\nglfwWindowHint(GLFW_CLIENT_API, GLFW_NO_API);\nglfwWindow = glfwCreateWindow(800, 600, \"Metal Engine\", NULL, NULL);\nif (!glfwWindow) {\nglfwTerminate();\nexit(EXIT_FAILURE);\n}\nmetalWindow = glfwGetCocoaWindow(glfwWindow);\nmetalLayer = [CAMetalLayer layer];\nmetalLayer.device = (__bridge id&lt;MTLDevice&gt;)metalDevice;\nmetalLayer.pixelFormat = MTLPixelFormatBGRA8Unorm;\nmetalWindow.contentView.layer = metalLayer;\nmetalWindow.contentView.wantsLayer = YES;\n}\n</code></pre> The <code>initWindow()</code> function first initializes GLFW, and uses <code>glfwWindowHint()</code> to tell GLFW not to create the OpenGL graphics context, as we're using Metal instead :). We set our <code>glfwWindow</code>, with a window width of <code>800</code> and height of <code>600</code>, and a window name of <code>Metal Engine</code>. If window creation fails, we exit.</p> <p>We then set our <code>metalWindow</code> with a call to <code>glfwGetCocoaWindow()</code>, specifying our <code>glfwWindow</code> as the input. This gives us a reference to the underlying native macOS Cocoa window that <code>GLFW</code> uses underneath for window management. We then use some funky Objective-C syntax <code>[CAMetalLayer layer]</code> to create a metalLayer, as the <code>metal-cpp</code> library does not yet expose all of the parts necessary to do this for some reason. We then set a device, using the <code>__bridge id&lt;MTLDevice&gt;</code> cast to convert from a C++ <code>MTL::Device*</code> pointer to an Objective-C <code>MTLDevice*</code>. We'll eventually end up using this bridging interface a little bit more to convert back and forth between Objective-C and C++ types. We then set the <code>pixelFormat</code> for the <code>metalLayer</code>, and give the <code>metalLayer</code> to the Cocoa window. </p> <p>By setting the layer of the <code>contentView</code> to our <code>metalLayer</code>, we're telling the Cocoa window underneath to use the Metal layer for rendering its content. The Metal layer will provide the low-level access to Metal that we need to perform our graphics rendering. Once the layer of the <code>contentView</code> is set, any content that we draw on the Metal layer will be displayed in the window. Essentially, the Metal layer is acting as what's called the \"framebuffer\". We also specify the <code>wantsLayer</code> property of the window's <code>contentView</code>, to ensure that the Cocoa window underneath will use the <code>metalLayer</code> for rendering its content.</p> <p>mtl_engine.mm<pre><code>void MTLEngine::run() {\nwhile (!glfwWindowShouldClose(glfwWindow)) {\nglfwPollEvents();\n}\n}\n</code></pre> Next, we have the <code>run()</code> function, which will eventually be our main program loop, where we'll handle our window updates and draw to the screen. The loop continues until <code>glfwWindowShouldClose(glfwWindow)</code> returns true, which for now will only happen when you hit the red close button at the top of the window. Every iteration of the loop, we'll need to call <code>glfwPollEvents()</code>, which we'll eventually use for handling keyboard input. If you don't include it now, the window won't launch!</p> <p>mtl_engine.mm<pre><code>void MTLEngine::cleanup() {\nglfwTerminate();\n}\n</code></pre> Our last function is the <code>cleanup()</code> function, where we'll eventually be handling our de-initiallisation and de-allocation of resources upon program close. For now, this will just include these two commands, <code>glfwTerminate()</code>, and <code>metalDevice-&gt;release()</code>. What is <code>release()</code> for? Glad you asked.</p>"},{"location":"Lesson%201%3A%20Hello%20Metal/1.%20Hello%20Window/#resource-management-with-metal-cpp","title":"Resource Management with metal-cpp","text":"<p>It's very important to understand how resource allocation works with metal-cpp. Any object created with a method whose name's begin with <code>alloc</code>, <code>new</code>, <code>copy</code>, <code>mutableCopy</code>, or <code>Create</code>, is owned by you. This means that you're responsible for de-allocation of the resource after you're done with it. Whenever an object is created using one of these aforementioned functions, it is assigned by default a <code>retainCount</code> of 1. When an objects <code>retainCount</code> becomes 0, the object is deallocated. Since we previously created our metalDevice with MTL::CreateSystemDefaultDevice, we're owners of the object, and we must release it when we're done with it. If you don't handle resource allocation properly, you can encounter crashes and memory-leaks. Bad stuff. You can read more about <code>metal-cpp's</code> memory allocation rules and policies in the <code>README.md</code> located in the <code>metal-cpp</code> folder, and the documentation it links to here and here.</p> <p>In order to instantiate our <code>MTLEngine</code>, head in to <code>main.mm</code> include the <code>mtl_engine.hpp</code> header, and create the engine: main.mm<pre><code>#include \"mtl_engine.hpp\"\nint main() {\nMTLEngine engine;\nengine.init();\nengine.run();\nengine.cleanup();\nreturn 0;\n}\n</code></pre></p> <p>If you build and run, you should get a beautiful blank looking window: </p>"},{"location":"Lesson%201%3A%20Hello%20Metal/2.%20Hello%20Triangle/","title":"Rendering a Triangle to the Screen","text":"<p>In this chapter, we'll finally be getting to render something to the screen! Exciting stuff. We're going to start with something simple: a triangle!</p>"},{"location":"Lesson%201%3A%20Hello%20Metal/2.%20Hello%20Triangle/#the-graphics-rendering-pipeline","title":"The Graphics Rendering Pipeline","text":"<p>Before we can render our triangle, it has to pass through something called the \"Graphics Pipeline\". The Graphics Pipeline is the series of steps that we must go through to render objects to our screen:</p> <p></p> <p>The steps listed in green are fully programmable by you. We write the code for the C++ application as well as the Vertex and Fragment Shader programs that are executed on the GPU. GPUs are massively parallel devices, and Shader programs are specifically designed to take advantage of this fact. Unlike with CPUs where you might have 4, 8, 16, or maybe even 32 individual cores, GPUs have hundrends and thousands of them, all running in parallel. Vertex shaders are designed to operate on the vertices of the meshes in our scene concurrently. Initially, our Vertex Shader will transform all 3 vertices of our triangle mesh at once, and this process will scale continously as we render more complicated scenes with more complicated meshes. A similar logic applies with the Fragment Shader. The Fragment Shader (sometimes called the \"Pixel\" Shader) operates on each pixel that we'll want to render, as determined by the output of the Rasterization stage. Modern displays have many individual pixels on them, which makes them a perfect candidate for GPU processing. The steps highlighted above in red are not programmable stages, but are instead configurable. We'll modify them as necessary in the following lessons. Here is a more detailed description of the steps involved in the Graphics Pipeline:</p> <ol> <li>The Application Stage: In our case, the \"Application Stage\" involves setting up our <code>MTLEngine</code>, intializing the Metal device, configuring a GLFW window, compiling shader code into a library, creating command buffers, encoding draw commands, managing the render loop, and more. This stage represents the C++ side of the process, preparing the environment for encoding GPU commands for rendering or compute related tasks.</li> <li>The Vertex Shader Stage: This is the stage in the pipeline where the GPU comes into play. In other Graphics APIs such as DirectX, typically the second stage of the Graphics Pipeline starts with something called the \"Input Assembler\", which is responsible for collecting and organizing vertex data from memory buffers on the GPU and feeding them into the rendering pipeline. In Metal, this functionality is integrated with the Vertex Shader stage. The Vertex Shader is responsible for processing vertex data, applying transformations, and passing the resulting data to the next stages of the rendering pipeline. To feed vertex data into the Vertex Shader in Metal, you define vertex descriptors that specify the layout and format of the vertex data. These descriptors are used to map vertex data from the vertex buffers to the input attributes of the Vertex Shader. This process effectively replaces the Input Assembler stage found in other Graphics APIs, streamlining the rendering pipeline in Metal.</li> <li>The Rasterization Stage: In Metal, the Rasterization Stage converts vector-based geometry (aka vertices) into pixel-based fragments. It processes transformed vertices from the Vertex Shader, assembles them into primitives, interpolates attributes, performs perspective division, clips vertices to the view frustum, and identifies the pixels we want to shade within the rendered geometry. This stage effectively prepares the pixels or \"fragments\" for further processing and shading in the Fragment Shader Stage.</li> <li>The Fragment Shader Stage: The Fragment Shader in Metal is responsible for processing and shading pixel-based fragments generated during rasterization. The Fragment Shader is executed for each pixel on the screen, and typically performs a series of mathematical operations to calculate the final color of the pixel, such as texture sampling, lighting and material property calculations, blending with the color of nearby pixels, etc. Once processed, the fragments are combined to create the final rendered image on the screen.</li> <li>The Blending Stage: The Blending stage in Metal is responsible for combining the output of the Fragment Shader stage with the existing data in the framebuffer, considering factors like transparency, translucency, and depth. It determines how the final color of each pixel is calculated by blending the incoming fragment color with the color already present in the framebuffer, ultimately producing the final rendered image on the screen.</li> </ol> <p>Additionally, we also have another type of shader called \"Compute Shaders\". A Compute Shader is a program that runs on the GPU, seperate from the traditional rendering pipeline. It is used to perform general-purpose computational tasks that are generally (but not always) unrelated to rendering, such as image and audio processing, phsysics simulation, and machine learning. Compute shaders provide for an incredibly high degree of parallelism, and allow a user to perform parallel operations and algorithms much faster than on the CPU. We'll get into more detail about how we can use these in the Advanced Chapters later on.</p> <p>Now that I've described the Graphics Pipeline and these things called Shaders, let's get to writing some.</p> <p>To render our triangle, we'll need to create a new shader file in our Xcode project called <code>triangle.metal</code>. Unlike other Graphics APIs, in Metal there is only one file extension for writing shaders. You use the function definition to define a function as a vertex, fragment, or compute function. In our new Shader file, we're going to write two shader functions, correspondingly titled <code>vertexShader()</code> and <code>fragmentShader()</code>: triangle.metal<pre><code>#include &lt;metal_stdlib&gt;\nusing namespace metal;\nvertex float4\nvertexShader(uint vertexID [[vertex_id]],\nconstant simd::float3* vertexPositions)\n{\nfloat4 vertexOutPositions = float4(vertexPositions[vertexID][0],\nvertexPositions[vertexID][1],\nvertexPositions[vertexID][2],\n1.0f);\nreturn vertexOutPositions;\n}\nfragment float4 fragmentShader(float4 vertexOutPositions [[stage_in]]) {\nreturn float4(182.0f/255.0f, 240.0f/255.0f, 228.0f/255.0f, 1.0f);\n}\n</code></pre> The Metal shading language is essentially builty on top of C++14, and has some restrictions. You can learn more about the restrictions in section 1.4.4 of the Metal Specification. We first include the Metal standard library, and add a <code>using namespace metal;</code>. We need to pre-pend our <code>vertexShader()</code> function definition with the <code>vertex</code> keyword to tell Metal that this is a vertex function. We'll be returning a float4, made up of the position of each vertex in the triangle. Shader programs run concurrently on the GPU, so each vertex we input will be processed simultaneously. To render our triangle, since we have three input vertices, we'll really have three instances of this <code>vertexShader()</code> function running at the same time. This is why the first take in the vertexID as input, so that we can know which vertex we're working on. The second input is our triangle's <code>vertexPositions</code> buffer. You'll notice the <code>constant</code> keyword prepended to our type definition. This is called an address space attribute, and it dictates the region of memory that the buffer is located in on the GPU. Quoting the Metal Specification directly, \"the <code>constant</code> address space name refers to buffer memory objects allocated from the device memory pool that are read-only\". Essentially, the buffer is allocated on the GPU in read-only memory, we aren't going to be able to change it. You can read more about address spaces in section 4.0 of the Metal Specification. We'll use our <code>vertexID</code> to index the proper Vertex per Shader, and we're going to cast our <code>vertexPositions</code> to a larger homogeneous vector type of <code>float4</code>, as Vertex functions are required to output positions in four-dimensional clip-space coordinates. Here's a great write-up on why that's necessary over at learnopengl.com. </p> <p>Similarly, our <code>fragmentShader()</code> will output a <code>float4</code> of color values at each triangle Vertex, and interpolate between them. Since we're just returning the same color at each Vertex, the whole triangle will be the same color, as there aren't any other colors to interpolate between. You can fill these with any value's between 0.0-1.0, Red, Green, Blue. The input <code>vertexPositions</code> to the <code>fragmentShader()</code> isn't necessary quite yet, but it's there as a demonstration on how you would pass the vertexPositions for use within the Fragment shader. We'll make use of it once we have a triangle rendering.</p> <p>mtl_engine.hpp<pre><code>#include &lt;simd/simd.h&gt;\nclass MTLEngine {\npublic:\nvoid init();\nvoid run();\nvoid cleanup();\nprivate:\nvoid initDevice();\nvoid initWindow();\nvoid createTriangle();\nvoid createDefaultLibrary();\nvoid createCommandQueue();\nvoid createRenderPipeline();\nvoid encodeRenderCommand(MTL::RenderCommandEncoder* renderEncoder);\nvoid sendRenderCommand();\nvoid draw();\nMTL::Device* metalDevice;\nGLFWwindow* glfwWindow;\nNSWindow* metalWindow;\nCAMetalLayer* metalLayer;\nCA::MetalDrawable* metalDrawable;\nMTL::Library* metalDefaultLibrary;\nMTL::CommandQueue* metalCommandQueue;\nMTL::CommandBuffer* metalCommandBuffer;\nMTL::RenderPipelineState* metalRenderPSO;\nMTL::Buffer* triangleVertexBuffer;\n};\n</code></pre> Here are the headers, member functions and variables that we'll be adding to our MTLEngine. It may seem like quite a lot of stuff is required to render a simple triangle, and in the grand scheme of things it is. But, I assure you, the effort required in Metal is far less than the other modern Graphics APIs, being DirectX12 and Vulkan. In Vulkan, it typically takes about 1000 lines of code to render a single triangle \ud83d\ude31. With Metal, it will only take us about 100 lines of code to get there, and we've already got 40 of them :). Get your boots strapped ladies and gentleman, it's time to dive in.</p> <p>The first addition is <code>createTriangle()</code>: mtl_engine.mm<pre><code>void MTLEngine::init() {\ninitDevice();\ninitWindow();\ncreateTriangle();\n}\n...\n</code></pre> <pre><code>...\nvoid MTLEngine::createTriangle() {\nsimd::float3 triangleVertices[] = {\n{-0.5f, -0.5f, 0.0f},\n{ 0.5f, -0.5f, 0.0f},\n{ 0.0f,  0.5f, 0.0f}\n};\ntriangleVertexBuffer = metalDevice-&gt;newBuffer(&amp;triangleVertices,\nsizeof(triangleVertices),\nMTL::ResourceStorageModeShared);\n}\n</code></pre> This goes into our <code>init()</code> function after we initialize our Metal device and GLFW window. Given that our goal is to render a triangle, we first define some triangle vertices with our <code>simd::float3</code> vector type included from Apple's <code>&lt;simd/simd.h&gt;</code> library. The <code>simd</code> library is kind of Apple's equivalent to the <code>glm</code> library if you've worked with Vulkan or OpenGL. In fact, you can use glm, however there are some important differences to take into account. We'll use the <code>simd</code> library throughout this tutorial series to make it easier on ourselves. </p> <p>We then ask our Metal device to create a new Metal Buffer, copying our triangle vertices over to the GPU. You can think of a MTL::Buffer object like a buffer of data in any programming language, it's simply used to store data. Metal doesn't know anything about the contents or layout of the data - you can customize it however you like. All it knows is the size of the buffer, as it's the second argument you pass in to <code>newBuffer()</code>. You might notice the third argument, <code>MTL::ResourceStorageModeShared</code>. This tells Metal to give both the CPU and GPU the ability to access the object. We'll discuss the other two types of storage modes later on.</p> <p>The next addition to our MTLEngine is <code>createDefaultLibrary()</code>: mtl_engine.mm<pre><code>void MTLEngine::init() {\ninitDevice();\ninitWindow();\ncreateTriangle();\ncreateDefaultLibrary();\n}\n...\n</code></pre> <pre><code>...\nvoid MTLEngine::createDefaultLibrary() {\nmetalDefaultLibrary = metalDevice-&gt;newDefaultLibrary();\nif(!metalDefaultLibrary){\nstd::cerr &lt;&lt; \"Failed to load default library.\";\nstd::exit(-1);\n}\n}\n</code></pre> This goes into our <code>init()</code> function after we create our triangle and copy it to the GPU. The first thing we're going to do is access our <code>metalDevice</code> to create a new default library. Xcode automatically compiles all Metal source files (ending in <code>.metal</code>) within an Xcode project into a single default library upon project compilation. In our case, because we're not compiling our Metal source code at runtime, our <code>MTL::Library*</code> object will give us access to all of our Metal source code from a single source, making it very simple to access and manage. If for any reason we're unable to load our compiled Metal binaries, we'll exit. If for any reason later on you get any errors here, make sure that your Metal source files are added as compile targets in the <code>Build Phases</code> section of the project.</p> <p>mtl_engine.mm<pre><code>void MTLEngine::init() {\ninitDevice();\ninitWindow();\ncreateTriangle();\ncreateDefaultLibrary();\ncreateCommandQueue();\n}\n...\n</code></pre> <pre><code>...\nvoid MTLEngine::createCommandQueue() {\nmetalCommandQueue = metalDevice-&gt;newCommandQueue();\n}\n</code></pre> We'll now ask our Metal device to create a new Command Queue for us. But first, what are these Command Queus and Buffers?</p>"},{"location":"Lesson%201%3A%20Hello%20Metal/2.%20Hello%20Triangle/#the-metal-flow","title":"The \"Metal Flow\"","text":"<p> This image describes what I call the \"Metal Flow\"1. The \"Metal Flow\" can be summarized as follows: To perform graphics tasks in Metal, we first create a \"Command Queue\", which acts as the railway tracks in the above image. Multiple railway cars, or \"Command Buffers\", can run on these tracks concurrently. Command Buffers store individual \"Commands\" that instruct the GPU, such as drawing a triangle. These commands are executed on the GPU using \"Shader Code\" within programs called Shaders, like we discussed previously.</p>"},{"location":"Lesson%201%3A%20Hello%20Metal/2.%20Hello%20Triangle/#command-queues-and-command-buffers","title":"Command Queues and Command Buffers","text":"<p>A Command Queue allows us to create Command Buffers, which essentially represent a chunk of work for the GPU. We use Command Buffers by encoding individual commands into them via something called a Command Encoder, like for setting and rendering our triangle vertex data, or telling the GPU to draw our primitives.</p> Command Queue Command Encoder Command Buffer <p>A Command Queue is responsible for creating Command Buffers, which represent a set of tasks for the GPU. Command Encoders are used to encode individual commands into these buffers, such as rendering and drawing operations. You can create multiple Command Buffers within a Command Queue, which can offer various benefits, including concurrent execution, improved resource usage, increased flexibility, and efficient render pass management. In essence, a single Command Queue allows the creation of Command Buffers to organize GPU tasks, with Command Encoders facilitating the encoding of commands into these buffers.</p>"},{"location":"Lesson%201%3A%20Hello%20Metal/2.%20Hello%20Triangle/#render-pipelines","title":"Render Pipelines","text":"<p>We're now going to create our Render Pipeline.</p> <p>A render pipeline is an object that encapsulates the GPU's rendering state, including shaders, vertex data, and other rendering settings. It specifies how the GPU processes vertex and fragment data to generate the final output image. Render pipelines are created and configured once, and then used repeatedly for rendering similar graphics objects. In this case, our render pipeline is setup to render individual triangles. It will contain the vertex and fragment functions that we defined earlier in our <code>triangle.metal</code> shader file. In Lesson two, we'll explore how we can create a second render pipeline to handle lights in our scene, using entirely seperate vertex and fragment shader functions from the ones our triangle uses.</p> <p>mtl_engine.mm<pre><code>void MTLEngine::init() {\ninitDevice();\ninitWindow();\ncreateTriangle();\ncreateDefaultLibrary();\ncreateCommandQueue();\ncreateRenderPipeline();\n}\n...\n</code></pre> <pre><code>...\nvoid MTLEngine::createRenderPipeline() {\nMTL::Function* vertexShader = metalDefaultLibrary-&gt;newFunction(NS::String::string(\"vertexShader\", NS::ASCIIStringEncoding));\nassert(vertexShader);\nMTL::Function* fragmentShader = metalDefaultLibrary-&gt;newFunction(NS::String::string(\"fragmentShader\", NS::ASCIIStringEncoding));\nassert(fragmentShader);\nMTL::RenderPipelineDescriptor* renderPipelineDescriptor = MTL::RenderPipelineDescriptor::alloc()-&gt;init();\nrenderPipelineDescriptor-&gt;setLabel(NS::String::string(\"Triangle Rendering Pipeline\", NS::ASCIIStringEncoding));\nrenderPipelineDescriptor-&gt;setVertexFunction(vertexShader);\nrenderPipelineDescriptor-&gt;setFragmentFunction(fragmentShader);\nassert(renderPipelineDescriptor);\nMTL::PixelFormat pixelFormat = (MTL::PixelFormat)metalLayer.pixelFormat;\nrenderPipelineDescriptor-&gt;colorAttachments()-&gt;object(0)-&gt;setPixelFormat(pixelFormat);\nNS::Error* error;\nmetalRenderPSO = metalDevice-&gt;newRenderPipelineState(renderPipelineDescriptor, &amp;error);\nrenderPipelineDescriptor-&gt;release();\n}\n</code></pre> We're first going to grab our two Metal functions from our <code>metalDefaultLibrary</code>, specifying the names of the corresponding Vertex and Fragment Shader functions. We then create a <code>MTL::RenderPipelineDescriptor</code> object, which allows us to to configure a variety of settings to use during a render pass. First we'll set the optional Label to give it a name, and then the Vertex and Fragment functions. We then set a Pixel format for our output image, which should match the format of our render target (aka our metalLayer). The color attachment in the <code>renderPipelineDescriptor</code> is used to specify the format and layout of the color buffer that will be used to store the output of our Fragment Shader. This buffer is where the final color information for each pixel of the rendered image is stored. We then use our <code>metalDevice</code> to represent a compiled Render Pipeline. We'll create a new <code>MTL::RenderPipelineState</code> object, specifying our <code>renderPipelineDescriptor</code> as input. In Metal, a render pipeline is a series of stages that process vertex and fragment data and produce a final image. Once we've created this pipeline state object, we can use it to render objects by encoding render commands into a Command Buffer and submitting it the GPU for drawing. You only have to create this object once, rather than each time we render a new frame. You can create a <code>MTL::RenderPipelineState</code> for each render pipeline that you may want. In our case, we'll just need this one. At the end of our function, we'll release our <code>renderPipelineDescriptor</code> object, as we're the owner of it (because we called <code>alloc()</code> to create it) and we won't be needing it anymore once the Render Pipeline has been created.</p> <p>With this, we've finished off our <code>init()</code> function. It's now time to move on to our <code>run()</code> loop.</p> <pre><code>void MTLEngine::run() {\nwhile (!glfwWindowShouldClose(glfwWindow)) {\n@autoreleasepool {\nmetalDrawable = (__bridge CA::MetalDrawable*)[metalLayer nextDrawable];\ndraw();\n}\nglfwPollEvents();\n}\n}\n</code></pre> <p>The first thing to note is the addition of our <code>@autoreleasepool</code>. The metal-cpp <code>README.md</code> describes the purpose of this better than I can, so i'll quote them here:</p> <p>Autorelease Pools and Objects</p> <p>Several methods that create temporary objects in *metal-cpp*** add them to an <code>AutoreleasePool</code> to help manage their lifetimes. In these situations, after *metal-cpp* creates the object, it adds it to an <code>AutoreleasePool</code>, which will release its objects when you release (or drain) it.</p> <p>By adding temporary objects to an AutoreleasePool, you do not need to explicitly call <code>release()</code> to deallocate them. Instead, you can rely on the <code>AutoreleasePool</code> to implicitly manage those lifetimes.</p> <p>If you create an object with a method that does not begin with <code>alloc</code>, <code>new</code>, <code>copy</code>, <code>mutableCopy</code>, or <code>Create</code>, the creating method adds the object to an autorelease pool.</p> <p>The typical scope of an <code>AutoreleasePool</code> is one frame of rendering for the main thread of the program. When the thread returns control to the RunLoop (an object responsible for receiving input and events from the windowing system), the pool is *drained*, releasing its objects.</p> <p>As such, our <code>AutoreleasePool</code> surrounds our <code>draw()</code> command, as they recommend. This will allow it to clean up some of the temporarily created objects for us each frame, like our <code>metalDrawable</code>, so we don't get any memory leaks.</p>"},{"location":"Lesson%201%3A%20Hello%20Metal/2.%20Hello%20Triangle/#drawing-render-passes-and-render-pipelines","title":"Drawing: Render Passes and Render Pipelines","text":"<p>In Metal, a render pass is a collection of rendering commands that takes a set of input resources (textures, buffers, etc.) and processes them through the graphics pipeline to produce an output, typically a rendered image. It groups related rendering operations performed on specific attachments (textures) to optimize the rendering process. To perform a render pass, you create a Command Encoder using the configured Render Pass Descriptor. The encoder is responsible for encoding draw commands, setting pipeline state, and providing resources (textures, buffers, etc.) to the graphics pipeline. Once you've encoded all your GPU commands, you end the encoding process, and the render pass is executed on the GPU when the command buffer is committed. </p> mtl_engine.cpp<pre><code>void MTLEngine::draw() {\nsendRenderCommand();\n}\nvoid MTLEngine::sendRenderCommand() {\nmetalCommandBuffer = metalCommandQueue-&gt;commandBuffer();\nMTL::RenderPassDescriptor* renderPassDescriptor = MTL::RenderPassDescriptor::alloc()-&gt;init();\nMTL::RenderPassColorAttachmentDescriptor* cd = renderPassDescriptor-&gt;colorAttachments()-&gt;object(0);\ncd-&gt;setTexture(metalDrawable-&gt;texture());\ncd-&gt;setLoadAction(MTL::LoadActionClear);\ncd-&gt;setClearColor(MTL::ClearColor(41.0f/255.0f, 42.0f/255.0f, 48.0f/255.0f, 1.0));\ncd-&gt;setStoreAction(MTL::StoreActionStore);\nMTL::RenderCommandEncoder* renderCommandEncoder = metalCommandBuffer-&gt;renderCommandEncoder(renderPassDescriptor);\nencodeRenderCommand(renderCommandEncoder);\nrenderCommandEncoder-&gt;endEncoding();\nmetalCommandBuffer-&gt;presentDrawable(metalDrawable);\nmetalCommandBuffer-&gt;commit();\nmetalCommandBuffer-&gt;waitUntilCompleted();\nrenderPassDescriptor-&gt;release();\n}\nvoid MTLEngine::encodeRenderCommand(MTL::RenderCommandEncoder* renderCommandEncoder) {\nrenderCommandEncoder-&gt;setRenderPipelineState(metalRenderPSO);\nrenderCommandEncoder-&gt;setVertexBuffer(triangleVertexBuffer, 0, 0);\nMTL::PrimitiveType typeTriangle = MTL::PrimitiveTypeTriangle;\nNS::UInteger vertexStart = 0;\nNS::UInteger vertexCount = 3;\nrenderCommandEncoder-&gt;drawPrimitives(typeTriangle, vertexStart, vertexCount);\n}\n</code></pre> <p>Our <code>draw()</code> command simply calls our <code>sendRenderCommand()</code> function, which actually handles the creation of our Render Pass. We first use our <code>metalCommandQueue</code> to create a Command Buffer. To encode render commands to a Command Buffer, we'll need a <code>MTL::RenderCommandEncoder</code>. Before we create one though, we'll need a <code>MTL::RenderPassDescriptor</code> , which serves to hold a collection of attachments for pixels generated by a render pass. The <code>colorAttachments</code> property specifies the textures or render targets that will be used to store the results of the render pass, hence why we give it the texture for our <code>metalDrawable</code>. We're going to also set a Clear Color, which is going to be the initial Color of all the pixels in the framebuffer. It'll essentially serve as the color of the background in our image. We then tell the GPU to store the rendered contents of our render pass to our <code>metalDrawable</code> texture with <code>MTL::StoreActionStore</code> when it's finished.</p> <p>We then create a <code>MTL::RenderCommandEncoder</code> to encode our render commands. Before we encode any commands, we need to tell the Command Encoder which Render Pipeline to process our commands in the context of. We set our Render Pipeline State to our previously created <code>metalRenderPSO</code> object, which defines our Triangle Rendering Pipeline. We then set our VertexBuffer to our <code>triangleVertexBuffer</code>, and tell it to draw the Triangle! With the Render Pipeline provided, the GPU will execute operations on the vetices of our triangle in the context of the vertex and fragment shader code we defined in our <code>triangle.metal</code> shader file. We then finish off our command encoding with <code>renderCommandEncoder-&gt;endEncoding()</code> to tell our Command Buffer that we're done issuing GPU commands. As a final command, we tell our Command Buffer to present the final Drawable, which will be the result of the command executed in the <code>renderCommandEncoder</code>. We then actually send these commands to the GPU with <code>metalCommandBuffer-&gt;commit()</code>. Critically, we tell the current thread to <code>waitUntilCompleted()</code>, which halts the execution of our C++ application until the GPU has finished its work. We'll discuss more complex methods of CPU&lt;-&gt;GPU synchronization later on. We release the <code>renderPassDescriptor</code> object, as we created it with the <code>alloc()</code> method.</p> <p>Finally, we can compile and run, and see our beautiful triangle displayed the the screen!</p>"},{"location":"Lesson%201%3A%20Hello%20Metal/2.%20Hello%20Triangle/#hello-triangle","title":"Hello Triangle!","text":"<p>If you look closely at the edges of the triangle, you'll notice that it might look a bit fuzzy. The reason for this is due to a bug in GLFW. By default, our <code>metalLayer</code> resolution gets set to the resolution we specified in <code>glfwCreateWindow()</code>. Normally this wouldn't be an issue, but in MacOS the supposed window size doesn't necessarily correspond to the size of the image rendered within our window, aka our metalLayer or 'framebuffer'. This issue occurs due to DPI scaling within MacOS. On a high resolution monitor like a 4K monitor, or even on a Macbook Air Display, MacOS by default renders the UI at a \"lower resolution\" so that it appears larger on the screen. That's why if you ask GLFW what it's window size is, it will report 800x600, the values that we specified in <code>glfwCreateWindow()</code>, but if you ask what its framebuffer resolution is with <code>glfwGetFramebufferSize()</code>, it will likely be larger than that. It all depends on the scaling you have set in System Preferences. Anways, to counteract this issue, we'll need to make a few changes to our GLFWwindow and metalLayer creation:</p> <p>mtl_engine.cpp<pre><code>void MTLEngine::initWindow() {\nglfwInit();\nglfwWindowHint(GLFW_CLIENT_API, GLFW_NO_API);\nglfwWindow = glfwCreateWindow(800, 600, \"Metal Engine\", NULL, NULL);\nif (!glfwWindow) {\nglfwTerminate();\nexit(EXIT_FAILURE);\n}\nint width, height;\nglfwGetFramebufferSize(glfwWindow, &amp;width, &amp;height);\nmetalWindow = glfwGetCocoaWindow(glfwWindow);\nmetalLayer = [CAMetalLayer layer];\nmetalLayer.device = (__bridge id&lt;MTLDevice&gt;)metalDevice;\nmetalLayer.pixelFormat = MTLPixelFormatBGRA8Unorm;\nmetalLayer.drawableSize = CGSizeMake(width, height);\nmetalWindow.contentView.layer = metalLayer;\nmetalWindow.contentView.wantsLayer = YES;\n}\n</code></pre> With this, our <code>metalLayer</code> size should now be set to the internal frameBuffer resolution of the <code>glfwWindow</code> object, which will give us nice and sharp edges:</p> <p></p> <ol> <li> <p>This image references the creation of Command Buffers, as described in the Apple Metal Docs article titled Setting Up a Command Structure.\u00a0\u21a9</p> </li> </ol>"},{"location":"Lesson%201%3A%20Hello%20Metal/3.%20Textures/","title":"Basic Texturing with Metal","text":""},{"location":"Lesson%201%3A%20Hello%20Metal/3.%20Textures/#handling-window-resizing","title":"Handling Window Resizing","text":"<p>Before we jump into the fun of adding textures, there's one thing I'd like to add, and that is window resizing. Currently when we resize the window, the resolution of our <code>metalLayer.drawableSize</code> will not be updated. To handle this, we'll first define two new functions in our <code>MTLEngine</code> class:</p> <p>mtl_engine.hpp<pre><code>class MTLEngine {\n...\nstatic void frameBufferSizeCallback(GLFWwindow *window, int width, int height);\nvoid resizeFrameBuffer(int width, int height);\n...\n};\n</code></pre> And then we'll actually implement them: mtl_engine.cpp<pre><code>void MTLEngine::frameBufferSizeCallback(GLFWwindow *window, int width, int height) {\nMTLEngine* engine = (MTLEngine*)glfwGetWindowUserPointer(window);\nengine-&gt;resizeFrameBuffer(width, height);\n}\nvoid MTLEngine::resizeFrameBuffer(int width, int height) {\nmetalLayer.drawableSize = CGSizeMake(width, height);\n}\nvoid MTLEngine::initWindow() {\n...\nglfwSetWindowUserPointer(glfwWindow, this);\nglfwSetFramebufferSizeCallback(glfwWindow, frameBufferSizeCallback);\n...\n}\n</code></pre> GLFW gives us the ability to define a callback function for when the window is resized. It populates the <code>GLFWwindow*</code>, <code>width</code>, and <code>height</code> automatically for us. One issue is that if the callback function is created as a member function in a class, it's required to be static. Normally, this would be an issue, as we want to update the <code>drawableSize</code> property of our <code>metalLayer</code> variable, which is non-static. We can solve this problem by using the <code>glfwSetWindowUserPointer()</code> and <code>glfwGetWindowUserPointer()</code> functions provided by GLFW. We can store a pointer to our <code>MTLEngine</code> instance in the GLFW window, then retrieve it in the static callback function to access our non-static <code>resizeFramebuffer()</code> function, where we actually resize the <code>metalLayer.drawableSize</code>.</p> <p>Now when we resize our window, it will resize the <code>metalLayer</code> accordingly!</p>"},{"location":"Lesson%201%3A%20Hello%20Metal/3.%20Textures/#applying-textures-to-our-meshes","title":"Applying Textures to our Meshes","text":"<p>In order to apply a texture to our mesh, we're going to to need a texture image. You can choose any image you like, but I'll be going with this one. Feel free to use it as well. Once you have your image, create an assets folder in your Xcode project and place your image inside it. Now that we have a texture, we'll need a way to load it in to memory. For this we'll use <code>stbi_image</code>, which is a lightweight header-only image loading library. In the <code>external</code> directory, create a new folder called <code>stb</code>, and right click this link and save the header file there. Next, create an implementation file called <code>stbi_image.cpp</code> in the same folder and add this code: stbi_image.cpp<pre><code>#define STB_IMAGE_IMPLEMENTATION\n#include \"stb_image.h\"\n</code></pre> We now need to instruct Xcode to link with our <code>stbi_image.cpp</code> implementation. Go to your Project Settings -&gt; Build Phases -&gt; Compile Sources and add <code>stbi_image.cpp</code>.</p> <p>To load our texture, we'll create a new <code>Texture()</code> class. Create two new files in the source directory: <code>Texture.hpp</code> and <code>Texture.cpp</code>. Texture.hpp<pre><code>#pragma once\n#include &lt;Metal/Metal.hpp&gt;\n#include &lt;stb/stb_image.h&gt;\nclass Texture {\npublic:\nTexture(const char* filepath, MTL::Device* metalDevice);\n~Texture();\nMTL::Texture* texture;\nint width, height, channels;\nprivate:\nMTL::Device* device;\n};\n</code></pre> We'll define a Constructor that takes in the filepath for the texture image we want to load, as well as our Metal Device so we can copy our image to the GPU as a <code>MTL::Texture</code>. We'll also define three public variables width, height, and channels to store the dimensions and number of color channels contained in our loaded image, as well as a handle to our Metal Device. Texture.cpp<pre><code>#include \"Texture.hpp\"\nTexture::Texture(const char* filepath, MTL::Device* metalDevice) {\ndevice = metalDevice;\nstbi_set_flip_vertically_on_load(true);\nunsigned char* image = stbi_load(filepath, &amp;width, &amp;height, &amp;channels, STBI_rgb_alpha);\nassert(image != NULL);\nMTL::TextureDescriptor* textureDescriptor = MTL::TextureDescriptor::alloc()-&gt;init();\ntextureDescriptor-&gt;setPixelFormat(MTL::PixelFormatRGBA8Unorm);\ntextureDescriptor-&gt;setWidth(width);\ntextureDescriptor-&gt;setHeight(height);\ntexture = device-&gt;newTexture(textureDescriptor);\nMTL::Region region = MTL::Region(0, 0, 0, width, height, 1);\nNS::UInteger bytesPerRow = 4 * width;\ntexture-&gt;replaceRegion(region, 0, image, bytesPerRow);\ntextureDescriptor-&gt;release();\nstbi_image_free(image);\n}\nTexture::~Texture() {\ntexture-&gt;release();\n}\n</code></pre> In our <code>Texture()</code> constructor, we first set our Metal Device handle, and then we tell stbi to flip our image vertically on load, as Metal expects the 0 coordinate on the y-axis to be on the bottom side of our image, rather than at the top.</p> <p>We then load our image, make sure the pointer isn't null, and then we create our <code>MTL::TextureDescriptor</code>, specifying the Pixel Format of the image, as well as the width and height. We then ask our device to create the texture for us with the specified parameters, and we then copy the image data into the texture buffer. Finally, we release the <code>textureDescriptor</code>, and free our <code>image</code> buffer. We now have a texture loaded into GPU memory! Hooray. Oh, and don't forget to create your destructor and release the <code>texture</code> on destruction.</p> <p>Now, we'll go back to our <code>mtl_engine.hpp</code> and make a few changes: mtl_engine.hpp<pre><code>...\n#include \"VertexData.hpp\"\n#include \"Texture.hpp\"\n#include &lt;stb/stb_image.h&gt;\n...\n#include &lt;filesystem&gt;\nclass MTLEngine {\npublic:\nvoid init();\nvoid run();\nvoid cleanup();\nprivate:\nvoid initDevice();\nvoid initWindow();\nvoid createSquare();\nvoid createDefaultLibrary();\nvoid createCommandQueue();\nvoid createRenderPipeline();\nvoid encodeRenderCommand(MTL::RenderCommandEncoder* renderEncoder);\nvoid sendRenderCommand();\nvoid draw();\nstatic void frameBufferSizeCallback(GLFWwindow *window, int width, int height);\nvoid resizeFrameBuffer(int width, int height);\nMTL::Device* metalDevice;\nGLFWwindow* glfwWindow;\nNSWindow* metalWindow;\nCAMetalLayer* metalLayer;\nCA::MetalDrawable* metalDrawable;\nMTL::Library* metalDefaultLibrary;\nMTL::CommandQueue* metalCommandQueue;\nMTL::CommandBuffer* metalCommandBuffer;\nMTL::RenderPipelineState* metalRenderPSO;\nMTL::Buffer* squareVertexBuffer;\nTexture* grassTexture;\n};\n</code></pre> The first thing to note is our new include files. You'll notice we haven't created <code>VertexData.hpp</code> yet, we'll get to that in a moment. We're no longer going to be rendering a triangle. Instead, we'll render a square, so change <code>createTriangle()</code> to <code>createSquare()</code>, and <code>triangleVertexBuffer</code> to <code>squareVertexBuffer</code>. Lastly, let's add our <code>grassTexture</code> member variable.</p> <p>As we're going to be rendering a texture, we're going to need to pass the GPU some extra information about how we'd like to map the texture to our square. We call this information \"uv\" or \"texture coordinates\". In this case, it will be very easy to create a mapping, as the four corners of our square will correspond directly to the four corners of our texture in uv coordinates. UV coordinates range from 0.0 to 1.0, with (0,0) corresponding to the bottom left-most corner of the texture image, and (1,1) corresponding to the top right-most corner. VertexData.hpp<pre><code>#pragma once\n#include &lt;simd/simd.h&gt;\nusing namespace simd;\nstruct VertexData {\nfloat4 position;\nfloat2 textureCoordinate;\n};\n</code></pre> Create a new file called <code>VertexData.hpp</code> and define a struct correspondingly named <code>VertexData</code>. You'll notice the inclusion of the <code>&lt;simd/simd.h&gt;</code> header file, which is an Apple library that contains math data-types that correspond directly with our Metal Shader datatypes, such as <code>float4</code> vectors, <code>float4x4</code> matrices, etc. We'll use this library to define our vertices as a <code>simd::float4</code>, and our texture coordinates as <code>simd::float2</code>. I've thrown in a <code>using namespace simd</code> for convenience.</p> <p>Now, we can hop back over to <code>mtl_engine.cpp</code> and define our square: mtl_engine.cpp<pre><code>void MTLEngine::createSquare() {\nVertexData squareVertices[] {\n{{-0.5, -0.5,  0.5, 1.0f}, {0.0f, 0.0f}},\n{{-0.5,  0.5,  0.5, 1.0f}, {0.0f, 1.0f}},\n{{ 0.5,  0.5,  0.5, 1.0f}, {1.0f, 1.0f}},\n{{-0.5, -0.5,  0.5, 1.0f}, {0.0f, 0.0f}},\n{{ 0.5,  0.5,  0.5, 1.0f}, {1.0f, 1.0f}},\n{{ 0.5, -0.5,  0.5, 1.0f}, {1.0f, 0.0f}}\n};\nsquareVertexBuffer = metalDevice-&gt;newBuffer(&amp;squareVertices, sizeof(squareVertices), MTL::ResourceStorageModeShared);\ngrassTexture = new Texture(\"assets/mc_grass.jpeg\", metalDevice);\n}\n</code></pre> You'll notice we're using six vertices in total to define the square, this is because our square is made up of two triangles, which share two vertices at the corners. Later, I'll cover indexed drawing, which will allow us to save precious memory resources by definining less vertices in total. For now though, this will do. We then of course want to create our <code>squareVertexBuffer</code>. Lastly, we'll create our <code>grassTexture</code> texture object. An important thing to note, is that if we want to specify a relative path to load the image like I've done here, we'll need to change our Xcode working directory to the project directory. We can do that by going to Product -&gt; Scheme -&gt; Edit Scheme, or by hitting <code>\u2318+&lt;</code>. Then go to Run -&gt; Options and click \"Use custom working directory:\" and specify <code>$(PROJECT_DIR)</code> as the directory. This should allow you to specify the correct path. Alternatively, you can specify the full-path to texture to load it if you like.</p> <p>mtl_engine.mm<pre><code>void MTLEngine::encodeRenderCommand(MTL::RenderCommandEncoder* renderCommandEncoder) {\nrenderCommandEncoder-&gt;setRenderPipelineState(metalRenderPSO);\nrenderCommandEncoder-&gt;setVertexBuffer(squareVertexBuffer, 0, 0);\nMTL::PrimitiveType typeTriangle = MTL::PrimitiveTypeTriangle;\nNS::UInteger vertexStart = 0;\nNS::UInteger vertexCount = 6;\nrenderCommandEncoder-&gt;setFragmentTexture(grassTexture-&gt;texture, 0);\nrenderCommandEncoder-&gt;drawPrimitives(typeTriangle, vertexStart, vertexCount);\n}\n</code></pre> Now we'll use our <code>renderCommandEncoder</code> to specify the buffer to load in our Vertex Shader for our square, and set the texture for use in our Fragment Shader.</p> <p>Finally, let's modify our Shader Code to be able to handle our Vertex Data and sample our texture. square.metal<pre><code>#include &lt;metal_stdlib&gt;\nusing namespace metal;\n#include \"VertexData.hpp\"\nstruct VertexOut {\n// The [[position]] attribute of this member indicates that this value\n// is the clip space position of the vertex when this structure is\n// returned from the vertex function.\nfloat4 position [[position]];\n// Since this member does not have a special attribute, the rasterizer\n// interpolates its value with the values of the other triangle vertices\n// and then passes the interpolated value to the fragment shader for each\n// fragment in the triangle.\nfloat2 textureCoordinate;\n};\nvertex VertexOut vertexShader(uint vertexID [[vertex_id]],\nconstant VertexData* vertexData) {\nVertexOut out;\nout.position = vertexData[vertexID].position;\nout.textureCoordinate = vertexData[vertexID].textureCoordinate;\nreturn out;\n}\nfragment float4 fragmentShader(VertexOut in [[stage_in]],\ntexture2d&lt;float&gt; colorTexture [[texture(0)]]) {\nconstexpr sampler textureSampler (mag_filter::linear,\nmin_filter::linear);\n// Sample the texture to obtain a color\nconst float4 colorSample = colorTexture.sample(textureSampler, in.textureCoordinate);\nreturn colorSample;\n}\n</code></pre> I mentioned in the previous chapter that the Metal Shading Language (MSL) is essentially C++14 with some limitations. This means we can include our new <code>VertexData</code> struct in our Shader code as well by including <code>VertexData.hpp</code>. </p> <p>As for our Vertex and Fragment shader functions, we'll need to rewrite them. I've defined a new struct <code>VertexOut</code>, which will hold our output data from the Vertex Shader. We need to declare the <code>[[position]]</code> attribute on our <code>position</code> member to indicate to Metal that it should apply perspective-division to it. Typically, our vertex-coordinate would be in clip-space by the end of our vertex function, but we have not yet applied perspective to our square. We will cover this in great detail in <code>Lesson 2</code> when we go 3D, so for now just know that Metal requires that we specify this attribute. Our second member, <code>textureCoordinate</code> doesn't require a special attribute. Our texture coordinates at each fragment in our square will be interpolated automatically by the rasterizer.</p> <p>Our new input into our <code>vertexShader()</code> function is of course our <code>vertexData</code>. This remains in the <code>constant</code> address space, which refers to buffers allocated in the read-only device memory pool. If you'd like to learn more about address spaces in Metal before I specifically cover them, please refer to section <code>4.0</code> of the Metal Shading Language Specification.</p> <p>As input to our <code>fragmentShader()</code> we'll pass in our <code>VertexOut</code> information, as we'll want the interpolated texture-coordinate information when sampling our texture. The <code>[[stage_in]]</code> attribute qualifier indicates that the <code>in</code> parameter is an input from the previous pipeline stage. Additionally, we'll also take in our texture as a <code>texture2d&lt;float&gt;</code>, specifying with the attribute <code>[[texture(0)]]</code> that we'll want the texture at index 0. Given that we only have one texture to sample from, we don't technically have to specify the texture slot, but it's a good practice to maintain. We first create a <code>sampler</code> object, specifying linear filtering for magnification and minification.</p> <ol> <li>Magnification: This occurs when a texture is displayed at a larger size on the screen than its original size (i.e., the texels cover more than one screen pixel). In this case, the shader needs to interpolate the color values between the texels to create a smooth transition between them and avoid a blocky appearance.</li> <li>Minification: This occurs when a texture is displayed at a smaller size on the screen than its original size (i.e., multiple texels are mapped to a single screen pixel). In this case, the shader needs to determine the best color value to represent the group of texels that are being combined into a single screen pixel.</li> </ol> <p>In the texture sampler, we specify linear for both mag_filter (magnification filter) and min_filter (minification filter), which means we want to use linear interpolation for both magnification and minification operations. Linear interpolation provides smoother and more natural transitions between the texels compared to nearest-neighbor filtering (which would result in a blocky or pixelated appearance). However, linear filtering may also introduce some blurriness, especially during minification.</p> <p>Finally, we sample and return the color from our texture, using our <code>textureSampler</code> and interpolated texture coordinate <code>in.textureCoordinate</code>.</p> <p>Before we'll be able to see our beautiful textured square, we'll have to make a couple of last changes in <code>mtl_engine.cpp</code>: mtl_engine.cpp<pre><code>void MTLEngine::encodeRenderCommand(MTL::RenderCommandEncoder* renderCommandEncoder) {\nrenderCommandEncoder-&gt;setRenderPipelineState(metalRenderPSO);\nrenderCommandEncoder-&gt;setVertexBuffer(squareVertexBuffer, 0, 0);\nMTL::PrimitiveType typeTriangle = MTL::PrimitiveTypeTriangle;\nNS::UInteger vertexStart = 0;\nNS::UInteger vertexCount = 6;\nrenderCommandEncoder-&gt;setFragmentTexture(grassTexture-&gt;texture, 0);\nrenderCommandEncoder-&gt;drawPrimitives(typeTriangle, vertexStart, vertexCount);\n}\n</code></pre> We'll set our <code>squareVertexBuffer</code>, update our vertex count to 6, and set our <code>grassTexture</code> at texture index 0 in our fragment shader. With this we can compile and run!</p> <p></p> <p>Yes yes, it does look like more of a rectangle than a square, but that's because our window is <code>800x600</code>. You can change it to <code>800x800</code> to make it square. We'll fix these scaling issues when we go 3D in Lesson 2.</p> <p>If you're stuck, you can download the source code for this chapter here.</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/","title":"Going 3D with Metal","text":"<p>Things will get a lot more interesting in this Chapter, as we're finally going 3D!</p> <p>To do this, we'll need to add 3 important things to our rendering engine:</p> <ol> <li>A 3D Object to Render. For this chapter, we'll render a cube.</li> <li>Perspective Projection to add a sense of depth to our scene.</li> <li>A Depth Buffer for discarding non-visible fragments.</li> </ol> <p>How will we create a cube? What is perspective projection? What is a... Depth Buffer? We'll discuss these topics in the following sections, as well as a few extra topics that will be useful for us as our scenes and objects get more complicated.</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#creating-a-cube","title":"Creating a Cube","text":"<p>A cube happens to be a perfect progression from the square that we rendered in the previous chapter. It's a simple object, so we can define the vertices manually.</p> <p>First, let's update the MTLEngine header file: mtl_engine.hpp<pre><code>class MTLEngine {\n...\nprivate:\n...\nMTL::Buffer* cubeVertexBuffer;\nMTL::Buffer* transformationBuffer;\n}\n</code></pre> First, let's open our <code>VertexData.hpp</code> file and add a struct called<code>TransformationData</code> that will hold our model, view, and perspective matrices. If you're not sure what those are, I'll get into it later in the chapter: VertexData.hpp<pre><code>#pragma once\n#include &lt;simd/simd.h&gt;\nusing namespace simd;\nstruct VertexData {\nfloat4 position;\nfloat2 textureCoordinate;\n};\nstruct TransformationData {\nfloat4x4 modelMatrix;\nfloat4x4 viewMatrix;\nfloat4x4 perspectiveMatrix;\n};\n</code></pre></p> <p>For now, we'll define all 36 of our cube vertices manually. Just like in the last chapter, each face has 6 vertices, because our square is defined with two triangles. Each triangle of course has 3 vertices each. So, 3 vertices per triangle, 2 per face, 6 faces in total. That's 36 vertices of fun :), Lucky for you, I've defined vertices and texture coordinates for you: mtl_engine.mm<pre><code>void MTLEngine::createCube() {\n// Cube for use in a right-handed coordinate system with triangle faces\n// specified with a Counter-Clockwise winding order.\nVertexData cubeVertices[] = {\n// Front face\n{{-0.5, -0.5, 0.5, 1.0}, {0.0, 0.0}},\n{{0.5, -0.5, 0.5, 1.0}, {1.0, 0.0}},\n{{0.5, 0.5, 0.5, 1.0}, {1.0, 1.0}},\n{{0.5, 0.5, 0.5, 1.0}, {1.0, 1.0}},\n{{-0.5, 0.5, 0.5, 1.0}, {0.0, 1.0}},\n{{-0.5, -0.5, 0.5, 1.0}, {0.0, 0.0}},\n// Back face\n{{0.5, -0.5, -0.5, 1.0}, {0.0, 0.0}},\n{{-0.5, -0.5, -0.5, 1.0}, {1.0, 0.0}},\n{{-0.5, 0.5, -0.5, 1.0}, {1.0, 1.0}},\n{{-0.5, 0.5, -0.5, 1.0}, {1.0, 1.0}},\n{{0.5, 0.5, -0.5, 1.0}, {0.0, 1.0}},\n{{0.5, -0.5, -0.5, 1.0}, {0.0, 0.0}},\n// Top face\n{{-0.5, 0.5, 0.5, 1.0}, {0.0, 0.0}},\n{{0.5, 0.5, 0.5, 1.0}, {1.0, 0.0}},\n{{0.5, 0.5, -0.5, 1.0}, {1.0, 1.0}},\n{{0.5, 0.5, -0.5, 1.0}, {1.0, 1.0}},\n{{-0.5, 0.5, -0.5, 1.0}, {0.0, 1.0}},\n{{-0.5, 0.5, 0.5, 1.0}, {0.0, 0.0}},\n// Bottom face\n{{-0.5, -0.5, -0.5, 1.0}, {0.0, 0.0}},\n{{0.5, -0.5, -0.5, 1.0}, {1.0, 0.0}},\n{{0.5, -0.5, 0.5, 1.0}, {1.0, 1.0}},\n{{0.5, -0.5, 0.5, 1.0}, {1.0, 1.0}},\n{{-0.5, -0.5, 0.5, 1.0}, {0.0, 1.0}},\n{{-0.5, -0.5, -0.5, 1.0}, {0.0, 0.0}},\n// Left face\n{{-0.5, -0.5, -0.5, 1.0}, {0.0, 0.0}},\n{{-0.5, -0.5, 0.5, 1.0}, {1.0, 0.0}},\n{{-0.5, 0.5, 0.5, 1.0}, {1.0, 1.0}},\n{{-0.5, 0.5, 0.5, 1.0}, {1.0, 1.0}},\n{{-0.5, 0.5, -0.5, 1.0}, {0.0, 1.0}},\n{{-0.5, -0.5, -0.5, 1.0}, {0.0, 0.0}},\n// Right face\n{{0.5, -0.5, 0.5, 1.0}, {0.0, 0.0}},\n{{0.5, -0.5, -0.5, 1.0}, {1.0, 0.0}},\n{{0.5, 0.5, -0.5, 1.0}, {1.0, 1.0}},\n{{0.5, 0.5, -0.5, 1.0}, {1.0, 1.0}},\n{{0.5, 0.5, 0.5, 1.0}, {0.0, 1.0}},\n{{0.5, -0.5, 0.5, 1.0}, {0.0, 0.0}},\n};\ncubeVertexBuffer = metalDevice-&gt;newBuffer(&amp;cubeVertices, sizeof(cubeVertices), MTL::ResourceStorageModeShared);\ntransformationBuffer = metalDevice-&gt;newBuffer(sizeof(TransformationData), MTL::ResourceStorageModeShared);\n// Make sure to change working directory to Metal-Tutorial root\n// directory via Product -&gt; Scheme -&gt; Edit Scheme -&gt; Run -&gt; Options\ngrassTexture = new Texture(\"assets/mc_grass.jpeg\", metalDevice);\n}\n</code></pre> For now, we're also going to create our transformationBuffer here, we'll clean that up later. Note the order in which I've specified the vertices here. More experienced folks might recognize that the vertices have been specified to have a Counter Clockwise (CCW) Winding Order.\u3000Essentially, winding order is the order in which we specify the vertices of a triangle. I've included a visual below to get a better idea of what that looks like:</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#clockwise-and-counter-clockwise-winding-orders","title":"Clockwise and Counter-Clockwise Winding Orders","text":"<p>A triangle has 2 sides. In our case, we will instruct Metal to use the Counter Clockwise triangle winding order. The vertex winding order will determine which \"face\" or side of a given triangle corresponds to the front. Metal can then use this information to determine if a triangle face is facing towards or away from us, and cull (not render) it correspondingly. If a triangle is facing toward us we want to render it, as it will be unobstructed. If it's facing away from us, we can't see it, so we don't want to render it. Try to visualise the cube that we're going to be rendering. At any given time, we can only see at most 3 sides of it. That means that the remaining non-visible sides can be culled by Metal. Vertex winding order is important, as it gives us a cheap way to determine which triangles we don't need to render.</p> <p>Another question to ask is, how do you know which winding order to choose? This depends on the coordinate system that you want to use for your rendering engine.</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#choosing-a-coordinate-system","title":"Choosing a Coordinate System","text":"<p>An important decision that we need to make when building a 3D rendering application is to choose a coordinate system to work with. What I mean by a \u201ccoordinate system\u201d is, within the space where our objects and camera exists, what direction do we want our x, y, and z axis to point? The typical Cartesian coordinate system that you might be familiar with, we like to think of in the world of computer graphics as a \u201cleft-handed\u201d coordinate system. In a left-handed coordinate system, the x-axis points to the right, the y-axis points up, and the z-axis points into your screen. If you hold your hand out in front of you and make an \u201cL\u201d shape with your thumb and index finger, these correspond to your x and y axis respectively. If you then point your middle finger out away from you, in the direction of your screen, this would correspond to the z-axis. If you do the same hand signal with your right hand, and then turn your hand around so that your middle finger points towards you, this should give you a visual representation of a \u201cright-handed\u201d coordinate system, where the positive z-axis (your middle finger) points towards us, out of the screen. Out of personal preference, I have decided to follow this coordinate convention throughout this tutorial series. Additionally, I\u2019ve provided a visual below of the two coordinate systems I\u2019ve described.</p> Left-handed Coordinate System Right-Handed Coordinate System <p>There are some important ramifications that we should be aware of when choosing to work with a right or left-handed coordinate system. </p> <ul> <li>Direction of the result of the cross-product of two vectors. When using a right-handed coordinate system, we follow something called the right-hand rule. As an example, image that we have two vectors of length one (aka, unit vectors) that lie on the x-axis and y-axis pictured in the right-handed coordinate system above: \\({V}_1 = (1, 0, 0)\\), \\({V}_2 = (0, 1, 0)\\). To get another unit vector that points in the direction of the z-axis, we want to do \\({V}_1 \\times {V}_2\\). Notice the order I've specified the cross product in here, try making a sweeping motion with your fingers from vector \\({V}_1\\) in the direction of vector \\({V}_2\\), it should match the counter-clockwise rotation displayed for the z-axis. This is the same thing that Metal will do behind the scenes to determine the front and back face triangles of your 3D models when we instruct it to use a Counter Clockwise winding order for our vertices.<ul> <li>The winding order, or order in which you specify your vertices can impact how the object is rendered. In a right-handed coordinate system, we typically use a counter-clockwise winding order for our vertices, which allows us to easily determine whether a face is \u201cfront-facing\u201d, facing towards us, or \u201cback-facing\u201d facing away from us, allowing Metal to easily discard and not render faces pointing away from us.</li> <li>The normal vector for a face is calculated using the cross product of two edge vectors from the polygon. Normals are essential for various rendering techniques such as lighting calculations.</li> </ul> </li> <li>Looking Direction: In a right-handed system, the camera typically looks down the negative z-axis by default, whereas in a left-handed system the camera typically looks down the positive z-axis.</li> <li>Transformation matrices will slightly differ, like the perspective projection matrix. Or, if you choose to rotate an object around a particular axis, the direction of rotation will be flipped in a right versus left handed system, as you can see in the diagram above.</li> </ul>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#transformations","title":"Transformations","text":"<p>Now that we have decided what kind of coordinate system we want to use, we need to discuss the series of transformations that our objects will go through to be rendered on the screen. Typically when you import a model into your rendering engine, the coordinates that are provided will center the object at the origin. Take our cube for example, we defined it to be centered at the origin. We call this initial coordinate space for each model, the aptly named \u201cmodel-space\u201d. If we imagine a scene in a game, it\u2019s usually going to have all sorts of objects scattered throughout the scene, perhaps looking something like in the image below:</p> <p></p> <p>This mean's that we need some way to be able to move our objects around in space. Luckily, we can use affine transformations in the form of four dimensional matrices to do this. Using these, we can easily scale, translate, rotate, and sheer our vertices in space. I'll get a little bit more into the specifics in the Mathematics section of this chapter. When we scale, manipulate, or otherwise move our objects from their initial position/orientation in \"model-space\" to our desired position within the world, we say that these objects are now in \"world-space\". That\u2019s great, we\u2019ve now oriented everything properly in our scene, but, how do we view it from our camera\u2019s perspective, like in the image above? Well, your camera, much like the objects in your scene will also have a position and an orientation within the world or \u201cworld-space\u201d. In order to view everything from our camera\u2019s perspective, we\u2019re going to apply another matrix transformation called the \u201cview\u201d or sometimes the \u201ccamera\u201d transformation. The key thing to realize here is that, we aren\u2019t actually moving around any of the objects in the scene with the view transformation. We are simply applying a change of basis transformation to our world-space coordinates, along with a translation to move the origin of our world to the camera\u2019s current position. This new \u201cview-space\u201d coordinate system is defined by three vectors that we define our camera to have, as well as the position of the camera in world-space:</p> <ol> <li>Forward Vector (F): Points in the direction the camera is looking.</li> <li>Right Vector (R): Points to the right of the camera.</li> <li>Up Vector (U): Points above the camera.</li> <li>Camera Position (P): The Position of the camera in World-Space.</li> </ol> <p>A common form for the view matrix \\( M_{view} \\) used in a right-handed coordinate system is as follows:</p> \\[ M_{view} = \\begin{pmatrix} R_x &amp; R_y &amp; R_z &amp; -R \\cdot P \\\\ U_x &amp; U_y &amp; U_z &amp; -U \\cdot P \\\\ -F_x &amp; -F_y &amp; -F_z &amp; F \\cdot P \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>The resulting view-space matrix will combine a change of basis matrix defined by our Forward, Right, and Up vectors, with a translation matrix that moves the origin of the world to the camera\u2019s position. Now that our vertices are in view-space, the coordinate-space defined by our camera's position and orientation, we are ready to apply the perspective projection matrix. The perspective projection matrix is defined by the coordinate-system that we originally chose to work with, being a right-handed coordinate system, as well as the field of view (fov) of our camera, the aspect-ratio of our window, as well as a near and far clipping plane. In essence, each vertex on each model in our scene will go through this series of transformations to end up as a position in clip-space:</p> \\[{P}_{clip}= {M}_{perspective} * {M}_{view} * {M}_{world} * {V}_{model} \\] <p>Once our vertex positions have been transformed to clip-space, our job is done, and Metal will clip or cull objects outside of the viewing frustum, and clip objects to the viewing frustum. It will then automatically perform the perspective or \"homogenous\" divide on each of our coordinates, transforming it to the final coordinate space called Normalized Device Coordinates. Well, what does all of that mean? We're going to get to that in the next section.</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#what-is-perspective-projection","title":"What is Perspective Projection?","text":"<p>With our coordinate system chosen and our Cube now defined, we have to discuss the topic of Perspective Projection. In the context of 3D Computer Graphics, perspective projection is a technique used to represent a three-dimensional object on a two-dimensional plane, being our computer screen, mimicking the way our eyes perceive depth and scale. Essentially, we'll want to map 3D points in the world to 2D points on a rendering surface called the \u201cviewing\u201d or \u201cprojection\u201d plane (which ends up being displayed on your computer screen), while maintaining the perception of \u201cdepth\". Objects that are farther away from the camera appear smaller on the viewing plane, and parallel lines converge to a single point in the distance, known as the vanishing point.</p> <p>Getting objects to appear on our screen and mimic the way they appear in real life is a pretty involved process. There are a lot of different steps to it, and if you aren\u2019t familiar with the concept of \u201cperspective projection\u201d within the context of a 3D graphics pipeline, don\u2019t worry about it too much. It may take a little while to wrap your head around the entire process, so you can always feel free to just follow along with the code and try to understand it as you go along.</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#various-types-of-projection-visualized","title":"Various Types of Projection Visualized","text":"<p> In reality, there are a variety of ways to map or \"project\" 3D points onto a 2D surface, and they have different purposes. In our case, we're trying to simulate how we would perceive things in real life using perspective projection. The further an object is away from us, the smaller we want it to appear on our screen, and vice versa as it gets closer to us. If we were an architect however, we might choose instead to use an orthographic projection to visualize our architectural drawings. Orthographic projections map our 3D points straight onto the viewing plane without any regard for the distance away, and so they are widely used in various fields such as engineering, architecture, design, and art for their ability to represent 3D objects accurately and without distortion. They are especially useful in situations where it's essential to maintain the exact dimensions and shapes of the objects being depicted.</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#isometric-projection-visualised","title":"Isometric Projection Visualised","text":""},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#perspective-projection-visualised","title":"Perspective Projection Visualised","text":""},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#mathematics-behind-perspective-projection","title":"Mathematics Behind Perspective Projection","text":"<p>Perspective projection can be represented mathematically by a 4D transformation matrix. We\u2019ll use this perspective projection matrix to transform a point from what are called \u201chomogenous\u201d coordinates to it\u2019s 2D screen position. In the context of 3D graphics, a vertex position in 3D space that would typically be represented with Cartesian coordinates (x, y, z) can be represented in 4D \u201chomogenous\u201d coordinates as (x, y, z, w). </p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#why-are-they-useful","title":"Why are they useful?","text":"<ul> <li>Unified Representation: Homogeneous coordinates allow for a unified representation of translation, rotation, scaling, and other affine transformations as matrix multiplications. Without our 4D homogeneous coordinates, translation of our 3D objects would not be possible through matrix multiplication alone.</li> <li>Perspective Projection: They are crucial for perspective transformations. The division by w (known as the homogeneous divide or the perspective divide) allows for points to be moved closer or farther away from the camera, effectively simulating perspective.</li> </ul> <p>The perspective projection matrix takes in a variety of parameters, such as the field of view of your camera, the aspect ratio of the window/screen that you are rendering too, as well as the nearZ and farZ clipping planes.  Based on these parameters, the perspective projection matrix creates what is called a \u201cviewing frustum\u201d, essentially a rectangular pyramid around your scene that represents what you can see, originating from the camera\u2019s location. When you apply this perspective projection transformation matrix to your vertices, it essentially warps this pyramid-like viewing frustum into a cube shape that we call \u201cclip-space\u201d. When you apply the perspective projection matrix, the range of your x, y and z coordinates will be between [-w, w]. The w-component of your homogenous coordinate is manipulated in such a way that Metal will then divide our x, y, and z coordinates by the w-component automatically to normalize the range of our x, y, and z coordinates to Normalized Device Coordinate (NDC) space. In Metal, NDC is a left-handed coordinate system that ranges from [-1,1] in the x and y direction, and from [0,1] in the z direction. Having this normalized box-like shape allows us to easily test whether vertices in our scene are in or outside of the box, and discard or \u201ccull\u201d them appropriately, as we don\u2019t want to render anything we can\u2019t see. If you have half of an object inside the NDC box and half outside, then it will \u201cclip\u201d the object to the edge of the box, creating new vertices for the object along the box's edge.</p> <p></p> <p>In our engine, we'll use a right-hand perpsective projection matrix that will take in the field of view specified as an angle in radians (e.g. 90 * PI/2), the aspect ratio of our window (width/height), as well as the distance of the near and far clipping planes from the camera: <pre><code>matrix_float4x4 matrix_perspective_right_hand(float fovyRadians, float aspect, float nearZ, float farZ) {\nfloat ys = 1 / tanf(fovyRadians * 0.5);\nfloat xs = ys / aspect;\nfloat zs = farZ / (nearZ - farZ);\nreturn matrix_make_rows(xs,  0,  0,          0,\n0, ys,  0,          0,\n0,  0, zs, nearZ * zs,\n0,  0, -1,          0 );\n}\n</code></pre></p> \\[ M_{perspective} = \\begin{pmatrix} xs &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; ys &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; zs &amp; nearZ \\times zs \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\end{pmatrix} \\]"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#putting-it-all-together","title":"Putting It All Together","text":"<p>I think it\u2019s best to explain the series of transformations that our vertices will go through in order to be projected onto the screen with an example. Let's imagine we have a square that we want to render. Imagine it is positioned initially at the origin, aligned with the \\(xz\\) plane. The 4 homogenous vertices defining the square are:</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#step-1-defining-the-square-in-model-space","title":"Step 1: Defining the Square in Model Space","text":"<ul> <li>\\( A_{model} = [ -1, 0, -1, 1 ] \\)</li> <li>\\( B_{model} = [  1, 0, -1, 1 ] \\)</li> <li>\\( C_{model} = [  1, 0,  1, 1 ] \\)</li> <li>\\( D_{model} = [ -1, 0,  1, 1 ] \\)</li> </ul>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#step-2-model-transformation-translation","title":"Step 2: Model Transformation (Translation)","text":"<p>We'll move the square down the negative \\(z\\)-axis by two units. This requires a translation operation, represented by a translation matrix:</p> \\[ M_{translation} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{pmatrix} \\] <p>Multiplying each point by this matrix, we get these translated points in world-space:</p> <ul> <li>\\( A_{world} = [ -1, 0, -3, 1] = M_{translation} \\times A_{model} \\)</li> <li>\\( B_{world} = [  1, 0, -3, 1] = M_{translation} \\times B_{model} \\)</li> <li>\\( C_{world} = [  1, 0, -1, 1] = M_{translation} \\times C_{model} \\)</li> <li>\\( D_{world} = [ -1, 0, -1, 1] = M_{translation} \\times D_{model} \\)</li> </ul>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#step-3-view-transformation","title":"Step 3: View Transformation","text":"<p>To construct a view matrix, you'll need three orthogonal unit vectors. A unit vector is a vector that has a magnitude of 1. In other words, it is a vector that points in a certain direction but has been normalized to have a length of 1.</p> <ol> <li>Forward Vector (F): Points in the direction the camera is looking.</li> <li>Right Vector (R): Points to the right of the camera.</li> <li>Up Vector (U): Points above the camera.</li> </ol> <p>You'll also need the camera's position vector \\( P \\).</p> <ol> <li>Camera Position (P): The Position of the camera in World-Space.</li> </ol> <p>A common form for the view matrix \\( M_{view} \\) used in a right-handed coordinate system is as follows:</p> \\[ M_{view} = \\begin{pmatrix} R_x &amp; R_y &amp; R_z &amp; -R \\cdot P \\\\ U_x &amp; U_y &amp; U_z &amp; -U \\cdot P \\\\ -F_x &amp; -F_y &amp; -F_z &amp; F \\cdot P \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>We'll define our Forward, Right, and Up vectors, as well as the camera position like so:</p> <ul> <li>\\(F = (1, 0, 0) \\)</li> <li>\\(R = (0, 1, 0) \\)</li> <li>\\(U = (0, 0,-1) \\)</li> <li>\\(P = (0, 1, 1) \\)</li> </ul> <p>This will give us the resulting view vector:</p> \\[ M_{view} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp;-1 \\\\ 0 &amp; 0 &amp; 1 &amp;-1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{pmatrix} \\] <p>When we apply this view transformation matrix to our square vertices we get:</p> <ul> <li>\\( A_{view} = [ -1, -1, -4, 1] = M_{view} \\times A_{world} \\)</li> <li>\\( B_{view} = [  1, -1, -4, 1] = M_{view} \\times B_{world} \\)</li> <li>\\( C_{view} = [  1, -1, -2, 1] = M_{view} \\times C_{world} \\)</li> <li>\\( D_{view} = [ -1, -1, -2, 1] = M_{view} \\times D_{world} \\)</li> </ul> <p>Remember, these are the coordinates of the square in the coordinate space oriented in the direction of the camera, where the camera's position is the new origin. The camera was positioned at \\(P = (0, 1, 1)\\) on the \\(z\\)-axis, so when we move the original origin to the camera's position, it's as though everything has been shifted in the opposite direction. Essentially, we move the square by one unit in the direction of the negative z-axis and negative y-axis.</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#step-4-perspective-transformation","title":"Step 4: Perspective Transformation","text":"<p>Let's use a simplified perspective projection matrix for this example. We'll ignore aspect ratio and the far plane, focusing only on the near plane, which is at \\(z = -1\\). However, to make perspective account for Metal's NDC, we need to modify the z-coordinates in the perspective transformation. Given the original perspective matrix:</p> \\[ M_{perspective} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\end{pmatrix} \\] <p>Our resulting perspective projection matrix becomes:</p> \\[ M_{Metal} = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -0.5 &amp; -0.5 \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\end{pmatrix} \\] <p>The third row third column's -0.5 scales the z-coordinate to half its value, and the -0.5 at the end of the third row translates the z-coordinate so it's in the range [0, 1] after the perspective divide.</p> <p>Multiplying each point by this matrix, we get:</p> <ul> <li>\\( A_{clip} = [ -1, -1,  1.5, 4 ] = M_{Metal} \\times A_{view} \\)</li> <li>\\( B_{clip} = [  1, -1,  1.5, 4 ] = M_{Metal} \\times B_{view} \\)</li> <li>\\( C_{clip} = [  1, -1,  0.5, 2 ] = M_{Metal} \\times C_{view} \\)</li> <li>\\( D_{clip} = [ -1, -1,  0.5, 2 ] = M_{Metal} \\times D_{view} \\)</li> </ul>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#step-5-transform-to-ndc","title":"Step 5: Transform to NDC","text":"<p>To transform our vertices to NDC, Metal will automatically divide their respective x, y and z components by their w component, leaving us with:</p> <ul> <li>\\( A_{NDC} = ( -0.25, -0.25,  0.375) \\)</li> <li>\\( B_{NDC} = (  0.25, -0.25,  0.375) \\)</li> <li>\\( C_{NDC} = (  0.5, -0.5,  0.25) \\)</li> <li>\\( D_{NDC} = ( -0.5, -0.5,  0.25) \\)</li> </ul> <p>After the perspective divide, the points are in normalized device coordinates. In this example, you can see that the \\( x \\) coordinates of points A and B are now closer to the origin (scaled down to \\( \\pm 0.25 \\)) compared to points C and D (\\( \\pm 0.5 \\)). This is because points A and B are positioned farther away in world-space from the camera (at \\( z = -3 \\), and so take on a larger w-component) compared to points C and D (at \\( z = -1 \\)). Notice how the division by the w component is what caused this \"squishing\" that perspective provides.</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#step-6-the-viewport-transform","title":"Step 6: The Viewport Transform","text":"<p>In Metal, the rasterizer stage is responsible for transforming our Normalized-device coordinates (NDC) into viewport coordinates, which correspond to pixels on our screen. The (x,y) coordinates in the viewport are measured in pixels, with the origin placed in the top-left corner of the viewport. Metal will take our NDC coordinates x and y values and scale them by the width and height of the screen respectively, to end up between (0, screen width) in the x-direction, and (0, screen height) in the y-direction:</p> \\[ x_{screen} = \\frac{width}{2} \\times x_{NDC} + 1 \\] \\[ y_{screen} = \\frac{height}{2} \\times y_{NDC} + 1 \\] <p>Below is a visual representation of the viewport coordinate system used in Metal, borrowed from the Metal Shading Language Specification: </p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#step-7-rasterization","title":"Step 7: Rasterization","text":"<p>After our square goes through this series of transformations to end up in NDC coordinates, the rasterizer stage transforms our vertices into screen space. The rasterizer then takes these vertices and determines which pixels on the screen they cover. This process is called rasterization. The result is a set of fragments. Each fragment corresponds to a potential pixel on the screen and carries with it interpolated data from the vertices, such as color, texture coordinates, and depth.</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#result","title":"Result","text":"<p>When we put it all together, our scene might look a little something like this:</p> <p></p> <p>Hopefully this walkthrough has helped demonstrate the effects of each of the steps that our vertices go through, including perspective projection and the the significance of the homogenous coordinates mathematically. </p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#continuing-with-the-code","title":"Continuing with the Code","text":"<p>For convenience, we're going to add two new files provided by Apple to our project: AAPLMathUtilities.cpp and AAPLMathUtilities.h. We'll include the header in <code>mtl_engine.hpp</code>: mtl_engine.hpp<pre><code>#include \"AAPLMathUtilities.h\"\n</code></pre> This provides us a variety of utility functions for common operations utilised in Computer Graphics such as matrix transformations, quaternion rotations, and creating perspective projection matrices. </p> <p>Since we've already defined our Cube and created our <code>cubeVertexBuffer</code> as well as our <code>transformationBuffer</code>, we can now define our model, view, and perspective projection matrices:</p> mtl_engine.mm<pre><code>void MTLEngine::encodeRenderCommand(MTL::RenderCommandEncoder* renderCommandEncoder) {\n// Moves the Cube 2 units down the negative Z-axis\nmatrix_float4x4 translationMatrix = matrix4x4_translation(0, 0,-1.0);\nfloat angleInDegrees = glfwGetTime()/2.0 * 45;\nfloat angleInRadians = angleInDegrees * M_PI / 180.0f;\nmatrix_float4x4 rotationMatrix = matrix4x4_rotation(angleInRadians, 0.0, 1.0, 0.0);\nmatrix_float4x4 modelMatrix = simd_mul(translationMatrix, rotationMatrix);\nsimd::float3 R = simd::float3 {1, 0, 0}; // Unit-Right\nsimd::float3 U = simd::float3 {0, 1, 0}; // Unit-Up\nsimd::float3 F = simd::float3 {0, 0,-1}; // Unit-Forward\nsimd::float3 P = simd::float3 {0, 0, 1}; // Camera Position in World Space\nmatrix_float4x4 viewMatrix = matrix_make_rows(R.x, R.y, R.z, dot(-R, P),\nU.x, U.y, U.z, dot(-U, P),\n-F.x,-F.y,-F.z, dot( F, P),\n0, 0, 0, 1);\nfloat aspectRatio = (metalLayer.frame.size.width / metalLayer.frame.size.height);\nfloat fov = 90 * (M_PI / 180.0f);\nfloat nearZ = 0.1f;\nfloat farZ = 100.0f;\nmatrix_float4x4 perspectiveMatrix = matrix_perspective_right_hand(fov, aspectRatio, nearZ, farZ);\nTransformationData transformationData = { modelMatrix, viewMatrix, perspectiveMatrix };\nmemcpy(transformationBuffer-&gt;contents(), &amp;transformationData, sizeof(transformationData));\nrenderCommandEncoder-&gt;setRenderPipelineState(metalRenderPSO);\nrenderCommandEncoder-&gt;setVertexBuffer(cubeVertexBuffer, 0, 0);\nrenderCommandEncoder-&gt;setVertexBuffer(transformationBuffer, 0, 1);\nMTL::PrimitiveType typeTriangle = MTL::PrimitiveTypeTriangle;\nNS::UInteger vertexStart = 0;\nNS::UInteger vertexCount = 36;\nrenderCommandEncoder-&gt;setFragmentTexture(grassTexture-&gt;texture, 0);\nrenderCommandEncoder-&gt;drawPrimitives(typeTriangle, vertexStart, vertexCount);\n}\n</code></pre> <p>Using the <code>AAPLMatchUtilities.h</code> library we just included, we first define a 4x4 translation matrix so we can move the cube 1 unit down the negative \\(z\\)-axis, as our camera will looking down the negative z-axis. To make our scene a little bit more interesting, we will also make the cube rotate over time. We first define an angle in degrees that changes over time by using <code>glfwGetTime()</code>, and then convert it to radians because our rotation matrix helper function expects radian angles as an input.</p> <p>The next step is to create our Model Matrix. When multiplying affine transformations together, we need to be careful about the order in which we specify them, as matrix multiplication is not associative. If you're unsure as to why, try thinking about what would happen to our vertsice if we first uncentered our object from the origin, and then rotated it.  You can also try playing with the code yourself, creating various transformations and playing with their order. If you are rusty on your matrices, or are unfamiliar with linear algebra in general, I highly recommend watching 3blue1brown's Linear Algebra series. In this case, we want to first rotate our cube, and then translate it. Unfortunately, <code>simd_mul()</code> doesn't really provide a way to tell what order we're multiplying in, but what we want in mathematical notation is:</p> \\[ M_{model} = M_{translation} * M_{rotation} \\] <p>We've now successfully defined our <code>modelMatrix</code>, which moves our objects in the scene to our desired position and orientation in world-space. We're now going to define our <code>viewMatrix</code>, which will effectively act as a change of basis matrix to put our vertices into a new coordinate space defined by the camera's right, up, and forward vectors, as well as it's position in  world-space.</p> <p>We'll first define our unit vectors required for the \"camera, our Right, Up, Forward vectors. We'll also define the Camera position in world-space. We'll then create our <code>viewMatrix</code> using the <code>matrix_make_rows()</code> utility function from our <code>AAPLMatchUtilities.h</code> header.</p> <p>mtl_engine.mm<pre><code>void MTLEngine::encodeRenderCommand(MTL::RenderCommandEncoder* renderCommandEncoder) {\n...\nsimd::float3 R = simd::float3 {1, 0, 0}; // Unit-Right\nsimd::float3 U = simd::float3 {0, 1, 0}; // Unit-Up\nsimd::float3 F = simd::float3 {0, 0,-1}; // Unit-Forward\nsimd::float3 P = simd::float3 {0, 0, 1}; // Camera Position in World Space\nmatrix_float4x4 viewMatrix = matrix_make_rows(R.x, R.y, R.z, dot(-R, P),\nU.x, U.y, U.z, dot(-U, P),\n-F.x,-F.y,-F.z, dot( F, P),\n0, 0, 0, 1);\n...\n}\n</code></pre> We'll then create our perspective projection matrix, getting the aspect-ratio from our metalLayer width and height, we'll set the fov at 90 degrees and convert it to radians, and set the nearZ clipping plane at 0.1 units away from the camera, and the farZ clipping plane at 100 units away. Also be sure to update the vertexCount to 36:</p> <pre><code>...\nfloat aspectRatio = (metalLayer.frame.size.width / metalLayer.frame.size.height);\nfloat fov = 90 * (M_PI / 180.0f);\nfloat nearZ = 0.1f;\nfloat farZ = 100.0f;\nmatrix_float4x4 perspectiveMatrix = matrix_perspective_right_hand(fov, aspectRatio,\nnearZ, farZ);\nTransformationData transformationData = { modelMatrix, viewMatrix, perspectiveMatrix };\nmemcpy(transformationBuffer-&gt;contents(), &amp;transformationData, sizeof(transformationData));\n...\nNS::UInteger vertexCount = 36;\n</code></pre> <p>Finally, we'll rename our <code>square.metal</code> file to <code>cube.metal</code>, as we're now rendering a cube, and update the code to apply our transformation matrices to each vertex of the cube: cube.metal<pre><code>vertex VertexOut vertexShader(uint vertexID [[vertex_id]],\nconstant VertexData* vertexData,\nconstant TransformationData* transformationData)\n{\nVertexOut out;\nout.position = transformationData-&gt;perspectiveMatrix * transformationData-&gt;viewMatrix * transformationData-&gt;modelMatrix * vertexData[vertexID].position;\nout.textureCoordinate = vertexData[vertexID].textureCoordinate;\nreturn out;\n}\n</code></pre></p> <p>With this we can launch our application an- OH NO! What's this??</p> <p></p> <p>That doesn't look quite right does it? I've put a different texture on the cube to make the effect very obvious. One of the issues is that we don't have what's called a \"depth buffer\". The other issue is that all the faces of our cube are being rendered, because we haven't told Metal what winding order we're using or whether to cull the corresponding front or back faces. The winding order issue is an easy fix, but let's start to fix this mess by integrating a depth buffer into our renderer.</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#setting-up-a-depth-buffer","title":"Setting up a Depth Buffer","text":"<p>A depth buffer, also known as a z-buffer, is an essential component in the land of computer graphics, particularly when rendering 3D scenes. Its primary purpose is to resolve visibility, i.e., to determine which objects (or parts of objects) are visible from a particular viewpoint and which are obscured by other objects. You should be able to understand this based on how our cube is rendering. The issue is, Metal is drawing the back face over the front face, which doesn't make any sense, but because we aren't using a depth buffer it doesn't know how to use the depth information to draw it correctly.</p> <p>After Metal applies the viewport transformation to our objects in Normalized-Device Coordinate space, the coordinates of the objects are in screen space, which means they have specific pixel locations on the screen and depth values relative to the camera. When the rendering process begins, the depth buffer is created, essentially just as a texture that is same resolution as the rendering target (in our case, the screen), and each value in the depth buffer is typically set to a maximum, like 1.0 which corresponds to maximum z-depth in our Normalized-Device Coordinate space. In the standard rendering pipeline, after rasterization, fragments are processed in the fragment shader and then undergo depth testing. </p> <p>After an individual fragment is processed by the fragment shader, the fragment undergoes depth testing, where the fragment's depth value is compared against the current value in the depth buffer for that fragments screen position. If the depth test passes, meaning the current fragment's z-value is lower than the current one, the depth buffer is updated with the fragment's depth value, and the fragment's color is then potentially written to the color buffer (after additional stages like blending). If however, the depth test fails, meaning that the depth value of the current fragment being processed is greater than the one in the depth buffer, the fragment is discarded, and its color isn't written into the color buffer.</p> <p>You might think it strange or inefficient that the fragment shader might do all of that processing for an individual fragment, only to then perform the depth test and discard the fragment all together. In certain cases, you would certainly be right. This is why modern GPUs implement an optimization called early-z or early depth testing, where the GPU might actually check the depth of fragments before they're processed by the fragment shader. But, this can't be used in situations where the fragment shader modifies the depth value, or to give a more concrete example, when you're working with transparent objects like a glass window. While opaque objects can simply overwrite the color of a pixel if they're closer to the camera, transparent objects need to blend their color with whatever is already in the color buffer. As a result, transparent fragments can't simply discard fragments behind them, making Early-Z less effective.</p> <p>You can see in the image below what a depth buffer actually looks like compared to the final rendered image on the screen. The bottom image represents the depth buffer, where depth is based on what we call a \"resolve\" value or \"R\" value. An R value of 1.0 corresponds to an infinite distance, or the white background. The closer the objects gets to the camera, the darker the images becomes and the smaller the R value. If you look at the two points I've select on the 3D Model below for example, the point further away has a larger R value of 0.9789, compared to the point that's closer to the camera of 0.8589.</p> <p></p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#multisample-anti-aliasing","title":"Multisample Anti-Aliasing","text":"<p>While we add our Depth Buffer, we're also going to implement Multisampled Anti-Aliasing (MSAA), which follows a very similar process to integrating our Depth Buffer into our renderer. MSAA is a technique used in computer graphics to improve image quality by reducing visual artifacts known as \"aliasing\". Aliasing occurs when the high-frequency details in an image can't be adequately represented at the resolution at which the image is sampled. This typically manifests as jagged edges or \"jaggies\" on objects in the image, especially along diagonal or curved lines. MSAA tries to solve this issue by sampling multiple color values within a single pixel and blending them together.</p> <p>Take a look at the diagram below, on the left we have a triangle that overlaps part of a pixel on the screen. The black dot in the center is the sampling point that the rasterizer will use to generate the color that gets passed onto the fragment shader for that pixel. In this case, the triangle doesn't overlap with it so the final color is white. If you look at the example on the right however, we now have 4 sample points (hence, multisampling), two that lay in the triangle and two that lay outside of it. The rasterizer passes on the 4 color values at each sample, stored in an MSAA texture, onto the fragment shader stage to apply further processing to each sample. The final pixel color is a weighted blend of the colors of the 4 sampling points. MSAA operates on all pixel of our image, but has a more visible impact at the edges of objects, which is generally where aliasing is most noticeable.</p> <p></p> <p>This image is borrowed from Real Time Rendering, 3rd Edition.</p> <p>Below is a comparison of the edges of our cube, without MSAA, and with 4x MSAA enabled. You should notice that the image on the right has smoother edges than on the left.</p> MSAA Disabled 4x MSAA <p>We're going to need to update our <code>MTLEngine</code> class by adding a view functions and member variables. In particular, we'll have a function <code>createBuffers</code> for allocating our transformation buffer on the GPU, and we'll need to create texture objects for our Depth Buffer and MSAA textures. We'll also abstract out our render pass descriptor creation to its own function, and have a function to recreate our Depth and MSAA textures when our window resizes. We're also going to create a variety of objects for holding our depth stencil state and depth/msaa textures, and abstracting out our renderpass descriptor:</p> mtl_engine.hpp<pre><code>class MTLEngine {\n...\nprivate:\nvoid createCube();\nvoid createBuffers();\n...\nvoid createDepthAndMSAATextures();\nvoid createRenderPassDescriptor();\n// Upon resizing, update Depth and MSAA Textures.\nvoid updateRenderPassDescriptor();\n...\nMTL::DepthStencilState* depthStencilState;\nMTL::RenderPassDescriptor* renderPassDescriptor;\nMTL::Texture* msaaRenderTargetTexture = nullptr;\nMTL::Texture* depthTexture;\nint sampleCount = 4;\n}\n</code></pre> <p>We're adding quite a few things here. The first functions we'll update are our <code>init()</code> and <code>cleanup()</code> functions:</p> <p>mtl_engine.mm<pre><code>void MTLEngine::init() {\ninitDevice();\ninitWindow();\ncreateCube();\ncreateBuffers();\ncreateDefaultLibrary();\ncreateCommandQueue();\ncreateRenderPipeline();\ncreateDepthAndMSAATextures();\ncreateRenderPassDescriptor();\n}\n...\nvoid MTLEngine::cleanup() {\nglfwTerminate();\ntransformationBuffer-&gt;release();\nmsaaRenderTargetTexture-&gt;release();\ndepthTexture-&gt;release();\nrenderPassDescriptor-&gt;release();\nmetalDevice-&gt;release();\ndelete grassTexture;\n}\n</code></pre> The function calls should be self explanatory here.</p> <p>We'll first use our newly added <code>createBuffers()</code> function to create our transformation buffer: mtl_engine.mm<pre><code>void MTLEngine::createBuffers() {\ntransformationBuffer = metalDevice-&gt;newBuffer(sizeof(TransformationData), MTL::ResourceStorageModeShared);\n}\n</code></pre></p> <p>Before we create our MSAA and Depth Buffer Textures, we need to update our render pipeline creation:</p> <p>mtl_engine.mm<pre><code>void MTLEngine::createRenderPipeline() {\nMTL::Function* vertexShader = metalDefaultLibrary-&gt;newFunction(NS::String::string(\"vertexShader\", NS::ASCIIStringEncoding));\nassert(vertexShader);\nMTL::Function* fragmentShader = metalDefaultLibrary-&gt;newFunction(NS::String::string(\"fragmentShader\", NS::ASCIIStringEncoding));\nassert(fragmentShader);\nMTL::RenderPipelineDescriptor* renderPipelineDescriptor = MTL::RenderPipelineDescriptor::alloc()-&gt;init();\nrenderPipelineDescriptor-&gt;setVertexFunction(vertexShader);\nrenderPipelineDescriptor-&gt;setFragmentFunction(fragmentShader);\nassert(renderPipelineDescriptor);\nMTL::PixelFormat pixelFormat = (MTL::PixelFormat)metalLayer.pixelFormat;\nrenderPipelineDescriptor-&gt;colorAttachments()-&gt;object(0)-&gt;setPixelFormat(pixelFormat);\nrenderPipelineDescriptor-&gt;setSampleCount(sampleCount);\nrenderPipelineDescriptor-&gt;setDepthAttachmentPixelFormat(MTL::PixelFormatDepth32Float);\nNS::Error* error;\nmetalRenderPSO = metalDevice-&gt;newRenderPipelineState(renderPipelineDescriptor, &amp;error);\nif (metalRenderPSO == nil) {\nstd::cout &lt;&lt; \"Error creating render pipeline state: \" &lt;&lt; error &lt;&lt; std::endl;\nstd::exit(0);\n}\nMTL::DepthStencilDescriptor* depthStencilDescriptor = MTL::DepthStencilDescriptor::alloc()-&gt;init();\ndepthStencilDescriptor-&gt;setDepthCompareFunction(MTL::CompareFunctionLessEqual);\ndepthStencilDescriptor-&gt;setDepthWriteEnabled(true);\ndepthStencilState = metalDevice-&gt;newDepthStencilState(depthStencilDescriptor);\nrenderPipelineDescriptor-&gt;release();\nvertexShader-&gt;release();\nfragmentShader-&gt;release();\n}\n</code></pre> We first set the sample count that our render pipeline will use, as this parameter must be consistent across all pipeline stages and resources that are involved in multi-sampling. This includes MSAA render targets, depth buffers, and the render pipeline itself. The Depth Attachment needs to be compatible with the depth texture that we will create. In addition, we want to make sure to use high precision values in our depth buffer, so we'll set it to <code>MTL::PixelFormatDepth32Float</code>. We'll also add in some error handline for our PSO, , and create a depth stencil descriptor. We'll set our depth compare function to <code>MTL::CompareFunctionLessEqual</code>, so the depth test will pass if the current depth is closer or the same. We're also going to allow the gpu to write to the depth buffer, with <code>setDepthWriteEnabled(true)</code>, since we're rendering only opaque objects here.</p> <p>Next, we'll handle the creation of our MSAA and Depth Buffer Textures:</p> mtl_engine.mm<pre><code>void MTLEngine::createDepthAndMSAATextures() {\nMTL::TextureDescriptor* msaaTextureDescriptor = MTL::TextureDescriptor::alloc()-&gt;init();\nmsaaTextureDescriptor-&gt;setTextureType(MTL::TextureType2DMultisample);\nmsaaTextureDescriptor-&gt;setPixelFormat(MTL::PixelFormatBGRA8Unorm);\nmsaaTextureDescriptor-&gt;setWidth(metalLayer.drawableSize.width);\nmsaaTextureDescriptor-&gt;setHeight(metalLayer.drawableSize.height);\nmsaaTextureDescriptor-&gt;setSampleCount(sampleCount);\nmsaaTextureDescriptor-&gt;setUsage(MTL::TextureUsageRenderTarget);\nmsaaRenderTargetTexture = metalDevice-&gt;newTexture(msaaTextureDescriptor);\nMTL::TextureDescriptor* depthTextureDescriptor = MTL::TextureDescriptor::alloc()-&gt;init();\ndepthTextureDescriptor-&gt;setTextureType(MTL::TextureType2DMultisample);\ndepthTextureDescriptor-&gt;setPixelFormat(MTL::PixelFormatDepth32Float);\ndepthTextureDescriptor-&gt;setWidth(metalLayer.drawableSize.width);\ndepthTextureDescriptor-&gt;setHeight(metalLayer.drawableSize.height);\ndepthTextureDescriptor-&gt;setUsage(MTL::TextureUsageRenderTarget);\ndepthTextureDescriptor-&gt;setSampleCount(sampleCount);\ndepthTexture = metalDevice-&gt;newTexture(depthTextureDescriptor);\nmsaaTextureDescriptor-&gt;release();\ndepthTextureDescriptor-&gt;release();\n}\n</code></pre> <p>We first create our descriptors, and set various parameters such for each:</p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#msaa-texture","title":"MSAA Texture","text":"<ol> <li>Texture Type (TextureType2DMultisample): Specifies that this texture is a 2D texture with multiple samples per pixel, which is crucial for enabling MSAA.</li> <li>Pixel Format (PixelFormatBGRA8Unorm): Determines the format of the color data in the texture. BGRA8Unorm means it uses 8 bits for each of the blue, green, red, and alpha channels, and the data is normalized.</li> <li>Width and Height: These set the dimensions of the texture, usually matching the dimensions of the output framebuffer or drawable area.</li> <li>Sample Count: This is the number of samples per pixel and is a key setting for MSAA. More samples typically result in better anti-aliasing but at the cost of performance and memory usage. Here, we're using 4 samples.</li> <li>Usage (TextureUsageRenderTarget): Specifies how the texture will be used. Setting it as a render target means it can be written into by the GPU during rendering.</li> </ol>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#depth-texture","title":"Depth Texture","text":"<ol> <li>Texture Type (TextureType2DMultisample): Same as with the MSAA texture, specifies a 2D texture with multiple samples, which is necessary if you're using MSAA.</li> <li>Pixel Format (PixelFormatDepth32Float): Specifies that this texture will store depth information in a 32-bit floating-point format.</li> <li>Width and Height: Again, these usually match the dimensions of your framebuffer or drawable area.</li> <li>Usage (TextureUsageRenderTarget): Indicates that this texture will be used as a target for rendering operations, specifically for storing depth information.</li> <li>Sample Count: Must match the sample count of the MSAA texture for correct depth testing.</li> </ol> <p>We'll also update our <code>resizeFrameBuffer()</code> and <code>initWindow()</code> functions:</p> <p>mtl_engine.mm<pre><code>void MTLEngine::resizeFrameBuffer(int width, int height) {\nmetalLayer.drawableSize = CGSizeMake(width, height);\n// Deallocate the textures if they have been created\nif (msaaRenderTargetTexture) {\nmsaaRenderTargetTexture-&gt;release();\nmsaaRenderTargetTexture = nullptr;\n}\nif (depthTexture) {\ndepthTexture-&gt;release();\ndepthTexture = nullptr;\n}\ncreateDepthAndMSAATextures();\nmetalDrawable = (__bridge CA::MetalDrawable*)[metalLayer nextDrawable];\nupdateRenderPassDescriptor();\n}\nvoid MTLEngine::initWindow() {\n...\nmetalDrawable = (__bridge CA::MetalDrawable*)[metalLayer nextDrawable];\n}\n</code></pre> We'll need to recreate our MSAA and Depth textures when we resize our window. We'll also need to recreate the drawable, and update our render pass descriptor with our freshly created textures!</p> <p>Additionally, note that at the end of our <code>initWindow()</code> function, we will actually need to first create a <code>metalDrawable</code> so that when we can properly create our MSAA/Drawable Textures for the first frame of our application.</p> <p>Next, we'll implement our <code>createRenderPassDescriptor()</code> and <code>updateRenderPassDescriptor()</code> functions:</p> <p>mtl_engine.mm<pre><code>void MTLEngine::createRenderPassDescriptor() {\nrenderPassDescriptor = MTL::RenderPassDescriptor::alloc()-&gt;init();\nMTL::RenderPassColorAttachmentDescriptor* colorAttachment = renderPassDescriptor-&gt;colorAttachments()-&gt;object(0);\nMTL::RenderPassDepthAttachmentDescriptor* depthAttachment = renderPassDescriptor-&gt;depthAttachment();\ncolorAttachment-&gt;setTexture(msaaRenderTargetTexture);\ncolorAttachment-&gt;setResolveTexture(metalDrawable-&gt;texture());\ncolorAttachment-&gt;setLoadAction(MTL::LoadActionClear);\ncolorAttachment-&gt;setClearColor(MTL::ClearColor(41.0f/255.0f, 42.0f/255.0f, 48.0f/255.0f, 1.0));\ncolorAttachment-&gt;setStoreAction(MTL::StoreActionMultisampleResolve);\ndepthAttachment-&gt;setTexture(depthTexture);\ndepthAttachment-&gt;setLoadAction(MTL::LoadActionClear);\ndepthAttachment-&gt;setStoreAction(MTL::StoreActionDontCare);\ndepthAttachment-&gt;setClearDepth(1.0);\n}\nvoid MTLEngine::updateRenderPassDescriptor() {\nrenderPassDescriptor-&gt;colorAttachments()-&gt;object(0)-&gt;setTexture(msaaRenderTargetTexture);\nrenderPassDescriptor-&gt;colorAttachments()-&gt;object(0)-&gt;setResolveTexture(metalDrawable-&gt;texture());\nrenderPassDescriptor-&gt;depthAttachment()-&gt;setTexture(depthTexture);\n}\n</code></pre> Here, we are abstracting our <code>renderPassDescriptor</code> creation out into it's own function, and creating it only once, whereas before we had its creation as part of our <code>sendRenderCommand()</code> function that was called every frame. </p> <p>We'll create a color attachment which will serve as our <code>msaaRenderTargetTexture</code>, as well as a depth attachment for our <code>depthTexture</code>. We'll also create a function to update our <code>renderPassDescriptor</code> object every frame, by setting the  <code>msaaRenderTargetTexture</code> and <code>depthTexture</code>, and we'll also set our resolve texture, our <code>metalDrawable-&gt;texture()</code>, which is the texture we want our MSAA sampling data to resolve to. In our case, the MSAA texture will have 4 samples per pixel, and it will blend these together into one pixel so that it can be stored in the <code>metalDrawable</code> texture, as the drawable is what's finally presented to the screen. Technically, we don't need to keep setting our MSAA and depth textures each frame, but I'm keeping it here anyways as the operation is basically free, and in the event that we resize our window, we can easily call this function to reset and resize these textures.</p> <p>Next, we can update <code>sendRenderCommand()</code>:</p> mtl_engine.mm<pre><code>void MTLEngine::sendRenderCommand() {\nmetalCommandBuffer = metalCommandQueue-&gt;commandBuffer();\nupdateRenderPassDescriptor();\nMTL::RenderCommandEncoder* renderCommandEncoder = metalCommandBuffer-&gt;renderCommandEncoder(renderPassDescriptor);\nencodeRenderCommand(renderCommandEncoder);\nrenderCommandEncoder-&gt;endEncoding();\nmetalCommandBuffer-&gt;presentDrawable(metalDrawable);\nmetalCommandBuffer-&gt;commit();\nmetalCommandBuffer-&gt;waitUntilCompleted();\n}\n</code></pre> <p>Here, we're simply having <code>MTLEngine::sendRenderCommand()</code> update our <code>renderPassDescriptor</code> object with the <code>updateRenderPassDescriptor()</code> function we just created , rather than recreating it every frame like we did previously.</p> <p>mtl_engine.mm<pre><code>void MTLEngine::encodeRenderCommand(MTL::RenderCommandEncoder* renderCommandEncoder) {\nmatrix_float4x4 translationMatrix = matrix4x4_translation(0, 0.0, 0);\nfloat angleInDegrees = glfwGetTime()/2.0 * 90;\nfloat angleInRadians = angleInDegrees * M_PI / 180.0f;\nmatrix_float4x4 rotationMatrix = matrix4x4_rotation(angleInRadians, 0.0, -1.0, 0.0);\nmatrix_float4x4 modelMatrix = matrix_identity_float4x4;\nmodelMatrix = simd_mul(translationMatrix, rotationMatrix);\nmatrix_float4x4 viewMatrix = matrix4x4_translation(0.0, 0.0, 2.0);\nfloat aspectRatio = (metalLayer.frame.size.width / metalLayer.frame.size.height);\nfloat fov = 90 * (M_PI / 180.0f);\nfloat nearZ = 0.1f;\nfloat farZ = 100.0f;\nmatrix_float4x4 perspectiveMatrix = matrix_perspective_left_hand(fov, aspectRatio, nearZ, farZ);\nTransformationData transformationData = { modelMatrix, viewMatrix, perspectiveMatrix };\nmemcpy(transformationBuffer-&gt;contents(), &amp;transformationData, sizeof(transformationData));\n//    renderCommandEncoder-&gt;setTriangleFillMode(MTL::TriangleFillModeLines);\nrenderCommandEncoder-&gt;setRenderPipelineState(metalRenderPSO);\nrenderCommandEncoder-&gt;setDepthStencilState(depthStencilState);\nrenderCommandEncoder-&gt;setVertexBuffer(cubeVertexBuffer, 0, 0);\nrenderCommandEncoder-&gt;setVertexBuffer(transformationBuffer, 0, 1);\nMTL::PrimitiveType typeTriangle = MTL::PrimitiveTypeTriangle;\nNS::UInteger vertexStart = 0;\nNS::UInteger vertexCount = 36;\nrenderCommandEncoder-&gt;setFragmentTexture(grassTexture-&gt;texture, 0);\nrenderCommandEncoder-&gt;drawPrimitives(typeTriangle, vertexStart, vertexCount);\n}\n</code></pre> Finally, in our <code>encodeRenderCommand()</code> function, we are simply telling our <code>renderCommandEncoder</code> to set our Depth Stencil State that we created earlier, and voila, we should now have a 3D cube rendering with proper depth information taken into account and an image that has 4x MSAA applied.</p> <p></p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/1.%203D%20Cube/#discarding-unecessary-faces-and-fragments","title":"Discarding Unecessary Faces and Fragments","text":"<p>Even though we've implemented a depth buffer and MSAA into our renderer, we're still processing a lot of unecessary fragments. To demonstrate this, a look at the image below:</p> <p></p> <p>Remember how I said the rendering pipeline processes a fragment and then does does the depth test on it? When we render in wireframe mode, we can see that pretty clearly. We need to make sure we aren't processing all of those back faces and faces we can't see.</p> <p>mtl_engine.cpp<pre><code>void MTLEngine::encodeRenderCommand(MTL::RenderCommandEncoder* renderCommandEncoder) {\n...\nrenderCommandEncoder-&gt;setFrontFacingWinding(MTL::WindingCounterClockwise);\nrenderCommandEncoder-&gt;setCullMode(MTL::CullModeBack);\nrenderCommandEncoder-&gt;setTriangleFillMode(MTL::TriangleFillModeLines);\nrenderCommandEncoder-&gt;setRenderPipelineState(metalRenderPSO);\nrenderCommandEncoder-&gt;setDepthStencilState(depthStencilState);\n...\n}\n</code></pre> This is where the winding order finally comes into play. Remember that topic I discussed way back in the beginning of the chapter, and how we specifically specified our cube vertices with a Counter Clockwise Winding order? Well, we can give that information to Metal by telling the <code>renderCommandEncoder</code> to set the front face winding to <code>MTL::WindingCounterClockwise</code>, and to set the cull mode to <code>MTL::CullModeBack</code> so we can skip rendering those interior faces on the cube. Also notice here that I've set the triangle fill mode to <code>MTL::TriangleFillModeLines</code> so we can see the object in \"wire-frame\" mode. You can set it to <code>MTL::TriangleFillModeFill</code> to render like normal, or just leave it out and it will set that by default. And voila, we have a beautifully rendered wire-frame view of our cube, discarding the faces we can't see :D</p> <p></p> <p>The code for this chapter is accessible on the GitHub repository under branch Lesson_2_1.</p> <p></p> <ol> <li> <p>This image references Figure 1 in Section 1.6 of the Metal Shading Language Specification 1.6 Metal Coordinate Systems.\u00a0\u21a9</p> </li> </ol>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/2.%20Basic%20Lighting/","title":"Lighting","text":"<p>Full write-up soon to come! For now, the code is accessible on the GitHub repository under branch Lesson_2_2, and here is the finished product:</p> <p></p>"},{"location":"Lesson%202%3A%20Lets%20Go%203D%21/3.%20Loading%203D%20Models/","title":"Loading Models","text":"<p>Full write-up soon to come! For now, the code is accessible on the GitHub repository under branch Lesson_2_3, and here are some sample images:</p> <p> </p>"}]}